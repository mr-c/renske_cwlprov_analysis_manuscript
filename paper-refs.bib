@article{afganGalaxyPlatformAccessible2016,
  title = {The {{Galaxy}} Platform for Accessible, Reproducible and Collaborative Biomedical Analyses: 2016 Update},
  shorttitle = {The {{Galaxy}} Platform for Accessible, Reproducible and Collaborative Biomedical Analyses},
  author = {Afgan, Enis and Baker, Dannon and {van~den~Beek}, Marius and Blankenberg, Daniel and Bouvier, Dave and {\v C}ech, Martin and Chilton, John and Clements, Dave and Coraor, Nate and Eberhard, Carl and Gr{\"u}ning, Bj{\"o}rn and Guerler, Aysam and {Hillman-Jackson}, Jennifer and Von~Kuster, Greg and Rasche, Eric and Soranzo, Nicola and Turaga, Nitesh and Taylor, James and Nekrutenko, Anton and Goecks, Jeremy},
  year = {2016},
  month = jul,
  journal = {Nucleic Acids Research},
  volume = {44},
  number = {W1},
  pages = {W3-W10},
  issn = {0305-1048, 1362-4962},
  doi = {10.1093/nar/gkw343},
  urldate = {2022-05-06},
  abstract = {High-throughput data production technologies, particularly `next-generation' DNA sequencing, have ushered in widespread and disruptive changes to biomedical research. Making sense of the large datasets produced by these technologies requires sophisticated statistical and computational methods, as well as substantial computational power. This has led to an acute crisis in life sciences, as researchers without informatics training attempt to perform computation-dependent analyses. Since 2005, the Galaxy project has worked to address this problem by providing a framework that makes advanced computational tools usable by non experts. Galaxy seeks to make data-intensive research more accessible, transparent and reproducible by providing a Web-based environment in which users can perform computational analyses and have all of the details automatically tracked for later inspection, publication, or reuse. In this report we highlight recently added features enabling biomedical analyses on a large scale.},
  langid = {english},
  keywords = {Galaxy,workflow management system,workflows},
  file = {/Users/r.d.wit/Zotero/storage/J3CWVUTP/Afgan e.a. - 2016 - The Galaxy platform for accessible, reproducible a.pdf}
}

@article{afganGalaxyPlatformAccessible2018,
  title = {The {{Galaxy}} Platform for Accessible, Reproducible and Collaborative Biomedical Analyses: 2018 Update},
  shorttitle = {The {{Galaxy}} Platform for Accessible, Reproducible and Collaborative Biomedical Analyses},
  author = {Afgan, Enis and Baker, Dannon and Batut, B{\'e}r{\'e}nice and {van~den~Beek}, Marius and Bouvier, Dave and {\v C}ech, Martin and Chilton, John and Clements, Dave and Coraor, Nate and Gr{\"u}ning, Bj{\"o}rn A and Guerler, Aysam and {Hillman-Jackson}, Jennifer and Hiltemann, Saskia and Jalili, Vahid and Rasche, Helena and Soranzo, Nicola and Goecks, Jeremy and Taylor, James and Nekrutenko, Anton and Blankenberg, Daniel},
  year = {2018},
  month = jul,
  journal = {Nucleic Acids Research},
  volume = {46},
  number = {W1},
  pages = {W537-W544},
  issn = {0305-1048, 1362-4962},
  doi = {10.1093/nar/gky379},
  urldate = {2022-05-06},
  langid = {english},
  keywords = {Galaxy,workflow management system,workflows},
  file = {/Users/r.d.wit/Zotero/storage/VUUULPD8/Afgan e.a. - 2018 - The Galaxy platform for accessible, reproducible a.pdf}
}

@article{aiCONTRIBUTORSKHALEDAMMAR,
  title = {{{CONTRIBUTORS KHALED AMMAR}}},
  author = {Ai, Borialis},
  pages = {10},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/RESJSI7J/Ai - CONTRIBUTORS KHALED AMMAR.pdf}
}

@article{akbarProgressChallengesMachine2022,
  title = {Progress and Challenges for the Machine Learning-Based Design of Fit-for-Purpose Monoclonal Antibodies},
  author = {Akbar, Rahmad and Bashour, Habib and Rawat, Puneet and Robert, Philippe A. and Smorodina, Eva and Cotet, Tudor-Stefan and {Flem-Karlsen}, Karine and Frank, Robert and Mehta, Brij Bhushan and Vu, Mai Ha and Zengin, Talip and {Gutierrez-Marcos}, Jose and {Lund-Johansen}, Fridtjof and Andersen, Jan Terje and Greiff, Victor},
  year = {2022},
  month = dec,
  journal = {mAbs},
  volume = {14},
  number = {1},
  pages = {2008790},
  issn = {1942-0862, 1942-0870},
  doi = {10.1080/19420862.2021.2008790},
  urldate = {2022-03-20},
  langid = {english},
  keywords = {antibodies,epitopes},
  file = {/Users/r.d.wit/Zotero/storage/NGZC27I9/Akbar e.a. - 2022 - Progress and challenges for the machine learning-b.pdf}
}

@inproceedings{alperEnhancingAbstractingScientific2013,
  title = {Enhancing and Abstracting Scientific Workflow Provenance for Data Publishing},
  booktitle = {Proceedings of the {{Joint EDBT}}/{{ICDT}} 2013 {{Workshops}} on - {{EDBT}} '13},
  author = {Alper, Pinar and Belhajjame, Khalid and Goble, Carole A. and Karagoz, Pinar},
  year = {2013},
  pages = {313},
  publisher = {{ACM Press}},
  address = {{Genoa, Italy}},
  doi = {10.1145/2457317.2457370},
  urldate = {2022-05-15},
  abstract = {Many scientists are using workflows to systematically design and run computational experiments. Once the workflow is executed, the scientist may want to publish the dataset generated as a result, to be, e.g., reused by other scientists as input to their experiments. In doing so, the scientist needs to curate such dataset by specifying metadata information that describes it, e.g. its derivation history, origins and ownership. To assist the scientist in this task, we explore in this paper the use of provenance traces collected by workflow management systems when enacting workflows. Specifically, we identify the shortcomings of such raw provenance traces in supporting the data publishing task, and propose an approach whereby distilled, yet more informative, provenance traces that are fit for the data publishing task can be derived.},
  isbn = {978-1-4503-1599-9},
  langid = {english},
  keywords = {data citation},
  file = {/Users/r.d.wit/Zotero/storage/H943Q3PM/Alper e.a. - 2013 - Enhancing and abstracting scientific workflow prov.pdf}
}

@article{alshahraniNeurosymbolicRepresentationLearning2017,
  title = {Neuro-Symbolic Representation Learning on Biological Knowledge Graphs},
  author = {Alshahrani, Mona and Khan, Mohammad Asif and Maddouri, Omar and Kinjo, Akira R and {Queralt-Rosinach}, N{\'u}ria and Hoehndorf, Robert},
  editor = {Kelso, Janet},
  year = {2017},
  month = sep,
  journal = {Bioinformatics},
  volume = {33},
  number = {17},
  pages = {2723--2730},
  issn = {1367-4803, 1367-4811},
  doi = {10.1093/bioinformatics/btx275},
  urldate = {2023-05-12},
  abstract = {Motivation: Biological data and knowledge bases increasingly rely on Semantic Web technologies and the use of knowledge graphs for data integration, retrieval and federated queries. In the past years, feature learning methods that are applicable to graph-structured data are becoming available, but have not yet widely been applied and evaluated on structured biological knowledge. Results: We develop a novel method for feature learning on biological knowledge graphs. Our method combines symbolic methods, in particular knowledge representation using symbolic logic and automated reasoning, with neural networks to generate embeddings of nodes that encode for related information within knowledge graphs. Through the use of symbolic logic, these embeddings contain both explicit and implicit information. We apply these embeddings to the prediction of edges in the knowledge graph representing problems of function prediction, finding candidate genes of diseases, protein-protein interactions, or drug target relations, and demonstrate performance that matches and sometimes outperforms traditional approaches based on manually crafted features. Our method can be applied to any biological knowledge graph, and will thereby open up the increasing amount of Semantic Web based knowledge bases in biology to use in machine learning and data analytics.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/3XFWM82E/Alshahrani et al. - 2017 - Neuro-symbolic representation learning on biologic.pdf}
}

@article{alterovitzEnablingPrecisionMedicine2018,
  title = {Enabling Precision Medicine via Standard Communication of {{HTS}} Provenance, Analysis, and Results},
  author = {Alterovitz, Gil and Dean, Dennis and Goble, Carole and Crusoe, Michael R. and {Soiland-Reyes}, Stian and Bell, Amanda and Hayes, Anais and Suresh, Anita and Purkayastha, Anjan and King, Charles H. and Taylor, Dan and Johanson, Elaine and Thompson, Elaine E. and Donaldson, Eric and Morizono, Hiroki and Tsang, Hsinyi and Vora, Jeet K. and Goecks, Jeremy and Yao, Jianchao and Almeida, Jonas S. and Keeney, Jonathon and Addepalli, KanakaDurga and Krampis, Konstantinos and Smith, Krista M. and Guo, Lydia and Walderhaug, Mark and Schito, Marco and Ezewudo, Matthew and Guimera, Nuria and Walsh, Paul and Kahsay, Robel and Gottipati, Srikanth and Rodwell, Timothy C. and Bloom, Toby and Lai, Yuching and Simonyan, Vahan and Mazumder, Raja},
  year = {2018},
  month = dec,
  journal = {PLOS Biology},
  volume = {16},
  number = {12},
  pages = {e3000099},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.3000099},
  urldate = {2022-08-05},
  langid = {english},
  keywords = {BCO,biocompute object,genomics,provenance},
  file = {/Users/r.d.wit/Zotero/storage/WU6IDPPD/Alterovitz e.a. - 2018 - Enabling precision medicine via standard communica.pdf}
}

@phdthesis{andreadisCapelinFastDataDriven2020,
  title = {Capelin: {{Fast Data-Driven Capacity Planning}} for {{Cloud Datacenters}}},
  author = {Andreadis, Georgios and Iosup, Alexandru and {van Beek}, Vincent},
  year = {2020}
}

@article{balakrishnanRecentAdvancesComputer,
  title = {Recent {{Advances In Computer Architecture}}: {{The}} Opportunities and Challenges for Provenance},
  author = {Balakrishnan, Nikilesh and Bytheway, Thomas and Carata, Lucian and Chick, Oliver R A and Snee, James and Akoush, Sherif and Sohan, Ripduman and Seltzer, Margo and Hopper, Andy},
  pages = {6},
  abstract = {In recent years several hardware and systems fields have made advances in technology that open new opportunities and challenges for provenance systems. In this paper we look at such technologies and discuss the implications they have for provenance. First, we discuss processor and memory controller technologies that enable fine-grained lineage capture, resulting in more precise and accurate provenance. Then, we look at programmable storage, 3D memory and co-processor technologies discussing how lineage capture in these heterogeneous environments results in richer and more complete provenance. We finally look at technological advances in the field of networking, namely NFV and SDN, discussing how these technologies enable easier provenance capture in the network.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/EX8HMYYZ/Balakrishnan e.a. - Recent Advances In Computer Architecture The oppo.pdf}
}

@article{barlowContinuousDiscontinuousProtein1986,
  title = {Continuous and Discontinuous Protein Antigenic Determinants},
  author = {Barlow, D. J. and Edwards, M. S. and Thornton, J. M.},
  year = {1986},
  month = aug,
  journal = {Nature},
  volume = {322},
  number = {6081},
  pages = {747--748},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/322747a0},
  urldate = {2022-08-22},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/V4V7PHGI/Barlow e.a. - 1986 - Continuous and discontinuous protein antigenic det.pdf}
}

@article{bechhoferWhyLinkedData2011,
  title = {Why Linked Data Is Not Enough for Scientists},
  author = {Bechhofer, Sean and Buchan, Iain and De Roure, David and Missier, Paolo and Ainsworth, John and Bhagat, Jiten and Couch, Philip and Cruickshank, Don and Delderfield, Mark and Dunlop, Ian and Gamble, Matthew and Michaelides, Danius and Owen, Stuart and Newman, David and Sufi, Shoaib and Goble, Carole},
  year = {2011},
  month = aug,
  journal = {Future Generation Computer Systems},
  volume = {29},
  number = {2},
  pages = {599--611},
  issn = {0167739X},
  doi = {10.1016/j.future.2011.08.004},
  urldate = {2023-02-18},
  abstract = {Scientific data represents a significant portion of the linked open data cloud and scientists stand to benefit from the data fusion capability this will afford. Publishing linked data into the cloud, however, does not ensure the required reusability. Publishing has requirements of provenance, quality, credit, attribution and methods to provide the reproducibility that enables validation of results. In this paper we make the case for a scientific data publication model on top of linked data and introduce the notion of Research Objects as first class citizens for sharing and publishing.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/8VZYV8C2/Bechhofer et al. - 2013 - Why linked data is not enough for scientists.pdf}
}

@article{belhajjameResearchObjectSuite2014,
  title = {The {{Research Object Suite}} of {{Ontologies}}: {{Sharing}} and {{Exchanging Research Data}} and {{Methods}} on the {{Open Web}}},
  shorttitle = {The {{Research Object Suite}} of {{Ontologies}}},
  author = {Belhajjame, Khalid and Zhao, Jun and Garijo, Daniel and Hettne, Kristina and Palma, Raul and Corcho, {\'O}scar and {G{\'o}mez-P{\'e}rez}, Jos{\'e}-Manuel and Bechhofer, Sean and Klyne, Graham and Goble, Carole},
  year = {2014},
  month = feb,
  journal = {arXiv:1401.4307 [cs]},
  eprint = {1401.4307},
  primaryclass = {cs},
  urldate = {2022-04-01},
  abstract = {Research in life sciences is increasingly being conducted in a digital and online environment. In particular, life scientists have been pioneers in embracing new computational tools to conduct their investigations. To support the sharing of digital objects produced during such research investigations, we have witnessed in the last few years the emergence of specialized repositories, e.g., DataVerse and FigShare. Such repositories provide users with the means to share and publish datasets that were used or generated in research investigations. While these repositories have proven their usefulness, interpreting and reusing evidence for most research results is a challenging task. Additional contextual descriptions are needed to understand how those results were generated and/or the circumstances under which they were concluded. Because of this, scientists are calling for models that go beyond the publication of datasets to systematically capture the life cycle of scientific investigations and provide a single entry point to access the information about the hypothesis investigated, the datasets used, the experiments carried out, the results of the experiments, the conclusions that were derived, the people involved in the research, etc.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Digital Libraries},
  file = {/Users/r.d.wit/Zotero/storage/6YR3Z6JA/Belhajjame e.a. - 2014 - The Research Object Suite of Ontologies Sharing a.pdf},
  doi = "10.48550/arXiv.1401.4307"
}

@article{belhajjameUsingSuiteOntologies2015,
  title = {Using a Suite of Ontologies for Preserving Workflow-Centric Research Objects},
  author = {Belhajjame, Khalid and Zhao, Jun and Garijo, Daniel and Gamble, Matthew and Hettne, Kristina and Palma, Raul and Mina, Eleni and Corcho, Oscar and {G{\'o}mez-P{\'e}rez}, Jos{\'e} Manuel and Bechhofer, Sean and Klyne, Graham and Goble, Carole},
  year = {2015},
  month = may,
  journal = {Journal of Web Semantics},
  volume = {32},
  pages = {16--42},
  issn = {15708268},
  doi = {10.1016/j.websem.2015.01.003},
  urldate = {2022-07-06},
  abstract = {Scientific workflows are a popular mechanism for specifying and automating data-driven in silico experiments. A significant aspect of their value lies in their potential to be reused. Once shared, workflows become useful building blocks that can be combined or modified for developing new experiments. However, previous studies have shown that storing workflow specifications alone is not sufficient to ensure that they can be successfully reused, without being able to understand what the workflows aim to achieve or to re-enact them. To gain an understanding of the workflow, and how it may be used and repurposed for their needs, scientists require access to additional resources such as annotations describing the workflow, datasets used and produced by the workflow, and provenance traces recording workflow executions.},
  langid = {english},
  keywords = {ontologies,related work,research object,RO,roevo,wfdesc,wfprov},
  file = {/Users/r.d.wit/Zotero/storage/3IJWK4MS/Belhajjame e.a. - 2015 - Using a suite of ontologies for preserving workflo.pdf}
}

@inproceedings{belhajjameWorkflowcentricResearchObjects2012,
  title = {Workflow-Centric Research Objects: {{First}} Class Citizens in Scholarly Discourse.},
  booktitle = {9th {{Extended Semantic Web Conference Hersonissos}}},
  author = {Belhajjame, Khalid and Corcho, Oscar and Garijo, Daniel and Zhao, Jun and Missier, Paolo and Newman, David and Palma, Raul and Bechhofer, Sean and Garc{\'i}a Cuesta, Esteban and {G{\'o}mez-P{\'e}rez}, Jos{\'e} Manuel and Klyne, Graham and Page, Kevin and Roos, Marco and Ruiz, Jos{\'e} Enrique and {Soiland-Reyes}, Stian and {Verdes-Montenegro}, Lourdes and De Roure, David and Goble, Carole A.},
  year = {2012},
  month = may,
  pages = {1--12},
  address = {{Hersonissos, Crete (Greece)}},
  urldate = {2023-02-10},
  abstract = {The Wf4Ever Research Object Model provides a vocabulary for the description of workflow-centric Research Objects: aggregations of resources relating to scientific workflows.},
  url = "http://ceur-ws.org/Vol-903/paper-01.pdf"
}

@article{bermanProteinDataBank2000,
  title = {The {{Protein Data Bank}}},
  author = {Berman, Helen M. and Westbrook, John and Feng, Zukang and Gilliland, Gary and Bhat, T.N. and Weissig, Helge and Shindyalov, Ilya N. and Bourne, Philip E.},
  year = {2000},
  month = jan,
  journal = {Nucleic Acids Research},
  volume = {28},
  number = {1},
  pages = {235--242},
  doi = {10.1093/nar/28.1.235},
  file = {/Users/r.d.wit/Zotero/storage/9PNYFPX6/Berman e.a. - 2000 - The Protein Data Bank.pdf}
}

@article{boettigerIntroductionDockerReproducible2015,
  title = {An Introduction to {{Docker}} for Reproducible Research},
  author = {Boettiger, Carl},
  year = {2015},
  month = jan,
  journal = {ACM SIGOPS Operating Systems Review},
  volume = {49},
  number = {1},
  pages = {71--79},
  issn = {0163-5980},
  doi = {10.1145/2723872.2723882},
  urldate = {2022-05-06},
  abstract = {As computational work becomes more and more integral to many aspects of scientific research, computational reproducibility has become an issue of increasing importance to computer systems researchers and domain scientists alike. Though computational reproducibility seems more straight forward than replicating physical experiments, the complex and rapidly changing nature of computer environments makes being able to reproduce and extend such work a serious challenge. In this paper, I explore common reasons that code developed for one research project cannot be successfully executed or extended by subsequent researchers. I review current approaches to these issues, including virtual machines and workflow systems, and their limitations. I then examine how the popular emerging technology Docker combines several areas from systems research - such as operating system virtualization, cross-platform portability, modular re-usable elements, versioning, and a `DevOps' philosophy, to address these challenges. I illustrate this with several examples of Docker use with a focus on the R statistical environment.},
  langid = {english},
  keywords = {Docker},
  file = {/Users/r.d.wit/Zotero/storage/D7Y67RUY/Boettiger - 2015 - An introduction to Docker for reproducible researc.pdf}
}

@article{bolyenReproducibleInteractiveScalable2019,
  title = {Reproducible, Interactive, Scalable and Extensible Microbiome Data Science Using {{QIIME}} 2},
  author = {Bolyen, Evan and Rideout, Jai Ram and Dillon, Matthew R. and Bokulich, Nicholas A. and Abnet, Christian C. and {Al-Ghalith}, Gabriel A. and Alexander, Harriet and Alm, Eric J. and Arumugam, Manimozhiyan and Asnicar, Francesco and Bai, Yang and Bisanz, Jordan E. and Bittinger, Kyle and Brejnrod, Asker and Brislawn, Colin J. and Brown, C. Titus and Callahan, Benjamin J. and {Caraballo-Rodr{\'i}guez}, Andr{\'e}s Mauricio and Chase, John and Cope, Emily K. and Da Silva, Ricardo and Diener, Christian and Dorrestein, Pieter C. and Douglas, Gavin M. and Durall, Daniel M. and Duvallet, Claire and Edwardson, Christian F. and Ernst, Madeleine and Estaki, Mehrbod and Fouquier, Jennifer and Gauglitz, Julia M. and Gibbons, Sean M. and Gibson, Deanna L. and Gonzalez, Antonio and Gorlick, Kestrel and Guo, Jiarong and Hillmann, Benjamin and Holmes, Susan and Holste, Hannes and Huttenhower, Curtis and Huttley, Gavin A. and Janssen, Stefan and Jarmusch, Alan K. and Jiang, Lingjing and Kaehler, Benjamin D. and Kang, Kyo Bin and Keefe, Christopher R. and Keim, Paul and Kelley, Scott T. and Knights, Dan and Koester, Irina and Kosciolek, Tomasz and Kreps, Jorden and Langille, Morgan G. I. and Lee, Joslynn and Ley, Ruth and Liu, Yong-Xin and Loftfield, Erikka and Lozupone, Catherine and Maher, Massoud and Marotz, Clarisse and Martin, Bryan D. and McDonald, Daniel and McIver, Lauren J. and Melnik, Alexey V. and Metcalf, Jessica L. and Morgan, Sydney C. and Morton, Jamie T. and Naimey, Ahmad Turan and {Navas-Molina}, Jose A. and Nothias, Louis Felix and Orchanian, Stephanie B. and Pearson, Talima and Peoples, Samuel L. and Petras, Daniel and Preuss, Mary Lai and Pruesse, Elmar and Rasmussen, Lasse Buur and Rivers, Adam and Robeson, Michael S. and Rosenthal, Patrick and Segata, Nicola and Shaffer, Michael and Shiffer, Arron and Sinha, Rashmi and Song, Se Jin and Spear, John R. and Swafford, Austin D. and Thompson, Luke R. and Torres, Pedro J. and Trinh, Pauline and Tripathi, Anupriya and Turnbaugh, Peter J. and {Ul-Hasan}, Sabah and {van der Hooft}, Justin J. J. and Vargas, Fernando and {V{\'a}zquez-Baeza}, Yoshiki and Vogtmann, Emily and {von Hippel}, Max and Walters, William and Wan, Yunhu and Wang, Mingxun and Warren, Jonathan and Weber, Kyle C. and Williamson, Charles H. D. and Willis, Amy D. and Xu, Zhenjiang Zech and Zaneveld, Jesse R. and Zhang, Yilong and Zhu, Qiyun and Knight, Rob and Caporaso, J. Gregory},
  year = {2019},
  month = aug,
  journal = {Nature Biotechnology},
  volume = {37},
  number = {8},
  pages = {852--857},
  issn = {1087-0156, 1546-1696},
  doi = {10.1038/s41587-019-0209-9},
  urldate = {2023-03-17},
  langid = {english},
  keywords = {BioSB Knowledge Graphs 2023 (WUR),Chengyao Peng,microbiome,provenance,TBR},
  file = {/Users/r.d.wit/Zotero/storage/MN7FWNX7/Bolyen et al. - 2019 - Reproducible, interactive, scalable and extensible.pdf}
}

@article{buddQuickGuideBuilding2015,
  title = {A {{Quick Guide}} for {{Building}} a {{Successful Bioinformatics Community}}},
  author = {Budd, Aidan and Corpas, Manuel and Brazas, Michelle D. and Fuller, Jonathan C. and Goecks, Jeremy and Mulder, Nicola J. and Michaut, Magali and Ouellette, B. F. Francis and Pawlik, Aleksandra and Blomberg, Niklas},
  editor = {Shehu, Amarda},
  year = {2015},
  month = feb,
  journal = {PLOS Computational Biology},
  volume = {11},
  number = {2},
  pages = {e1003972},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003972},
  urldate = {2022-04-19},
  abstract = {Scientific community'' refers to a group of people collaborating together on scientific-research-related activities who also share common goals, interests, and values. Such communities play a key role in many bioinformatics activities. Communities may be linked to a specific location or institute, or involve people working at many different institutions and locations. Education and training is typically an important component of these communities, providing a valuable context in which to develop skills and expertise, while also strengthening links and relationships within the community. Scientific communities facilitate: (i) the exchange and development of ideas and expertise; (ii) career development; (iii) coordinated funding activities; (iv) interactions and engagement with professionals from other fields; and (v) other activities beneficial to individual participants, communities, and the scientific field as a whole. It is thus beneficial at many different levels to understand the general features of successful, high-impact bioinformatics communities; how individual participants can contribute to the success of these communities; and the role of education and training within these communities. We present here a quick guide to building and maintaining a successful, high-impact bioinformatics community, along with an overview of the general benefits of participating in such communities. This article grew out of contributions made by organizers, presenters, panelists, and other participants of the ISMB/ECCB 2013 workshop ``The `How To Guide' for Establishing a Successful Bioinformatics Network'' at the 21st Annual International Conference on Intelligent Systems for Molecular Biology (ISMB) and the 12th European Conference on Computational Biology (ECCB).},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/733DN5NB/Budd e.a. - 2015 - A Quick Guide for Building a Successful Bioinforma.pdf}
}

@article{buddTenSimpleRules2015,
  title = {Ten {{Simple Rules}} for {{Organizing}} an {{Unconference}}},
  author = {Budd, Aidan and Dinkel, Holger and Corpas, Manuel and Fuller, Jonathan C. and Rubinat, Laura and Devos, Damien P. and Khoueiry, Pierre H. and F{\"o}rstner, Konrad U. and Georgatos, Fotis and Rowland, Francis and Sharan, Malvika and Binder, Janos X. and Grace, Tom and Traphagen, Karyn and Gristwood, Adam and Wood, Natasha T.},
  editor = {Bourne, Philip E.},
  year = {2015},
  month = jan,
  journal = {PLOS Computational Biology},
  volume = {11},
  number = {1},
  pages = {e1003905},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003905},
  urldate = {2022-04-19},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/ARTHSHXX/Budd e.a. - 2015 - Ten Simple Rules for Organizing an Unconference.pdf}
}

@article{capelMultitaskLearningLeverage2022,
  title = {Multi-Task Learning to Leverage Partially Annotated Data for {{PPI}} Interface Prediction},
  author = {Capel, Henriette and Feenstra, K. Anton and Abeln, Sanne},
  year = {2022},
  month = jun,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {10487},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-13951-2},
  urldate = {2023-02-23},
  abstract = {Abstract             Protein protein interactions (PPI) are crucial for protein functioning, nevertheless predicting residues in PPI interfaces from the protein sequence remains a challenging problem. In addition, structure-based functional annotations, such as the PPI interface annotations, are scarce: only for about one-third of all protein structures residue-based PPI interface annotations are available. If we want to use a deep learning strategy, we have to overcome the problem of limited data availability. Here we use a multi-task learning strategy that can handle missing data. We start with the multi-task model architecture, and adapted it to carefully handle missing data in the cost function. As related learning tasks we include prediction of secondary structure, solvent accessibility,~and buried residue. Our results show that the multi-task learning strategy significantly outperforms single task approaches. Moreover, only the multi-task strategy is able to effectively learn over a dataset extended with structural feature data, without additional PPI annotations. The multi-task setup becomes even more important, if the fraction of PPI annotations becomes very small: the multi-task learner trained on only one-eighth of the PPI annotations\textemdash with data extension\textemdash reaches the same performances as the single-task learner on all PPI annotations. Thus, we show that the multi-task learning strategy can be beneficial for a small training dataset where the protein's functional properties of interest are only partially annotated.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/DK8LQX89/Capel et al. - 2022 - Multi-task learning to leverage partially annotate.pdf}
}

@article{carataPrimerProvenance2014,
  title = {A Primer on Provenance},
  author = {Carata, Lucian and Akoush, Sherif and Balakrishnan, Nikilesh and Bytheway, Thomas and Sohan, Ripduman and Seltzer, Margo and Hopper, Andy},
  year = {2014},
  month = may,
  journal = {Communications of the ACM},
  volume = {57},
  number = {5},
  pages = {52--60},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/2596628},
  urldate = {2022-03-29},
  abstract = {Better understanding data requires tracking its history and context.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/M6434989/Carata e.a. - 2014 - A primer on provenance.pdf}
}

@inproceedings{chanProvMarkProvenanceExpressiveness2019,
  title = {{{ProvMark}}: {{A Provenance Expressiveness Benchmarking System}}},
  shorttitle = {{{ProvMark}}},
  booktitle = {Proceedings of the 20th {{International Middleware Conference}}},
  author = {Chan, Sheung Chi and Cheney, James and Bhatotia, Pramod and Pasquier, Thomas and Gehani, Ashish and Irshad, Hassaan and Carata, Lucian and Seltzer, Margo},
  year = {2019},
  month = dec,
  pages = {268--279},
  publisher = {{ACM}},
  address = {{Davis CA USA}},
  doi = {10.1145/3361525.3361552},
  urldate = {2022-05-15},
  abstract = {System level provenance is of widespread interest for applications such as security enforcement and information protection. However, testing the correctness or completeness of provenance capture tools is challenging and currently done manually. In some cases there is not even a clear consensus about what behavior is correct. We present an automated tool, ProvMark, that uses an existing provenance system as a black box and reliably identifies the provenance graph structure recorded for a given activity, by a reduction to subgraph isomorphism problems handled by an external solver. ProvMark is a beginning step in the much needed area of testing and comparing the expressiveness of provenance systems. We demonstrate ProvMark's usefuless in comparing three capture systems with different architectures and distinct design philosophies.},
  isbn = {978-1-4503-7009-7},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/H7JXCULA/Chan e.a. - 2019 - ProvMark A Provenance Expressiveness Benchmarking.pdf}
}

@inproceedings{chirigatiReproZipComputationalReproducibility2016,
  title = {{{ReproZip}}: {{Computational Reproducibility With Ease}}},
  shorttitle = {{{ReproZip}}},
  booktitle = {Proceedings of the 2016 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Chirigati, Fernando and Rampin, R{\'e}mi and Shasha, Dennis and Freire, Juliana},
  year = {2016},
  month = jun,
  pages = {2085--2088},
  publisher = {{ACM}},
  address = {{San Francisco California USA}},
  doi = {10.1145/2882903.2899401},
  urldate = {2022-08-04},
  abstract = {We present ReproZip, the recommended packaging tool for the SIGMOD Reproducibility Review. ReproZip was designed to simplify the process of making an existing computational experiment reproducible across platforms, even when the experiment was put together without reproducibility in mind. The tool creates a self-contained package for an experiment by automatically tracking and identifying all its required dependencies. The researcher can share the package with others, who can then use ReproZip to unpack the experiment, reproduce the findings on their favorite operating system, as well as modify the original experiment for reuse in new research, all with little effort. The demo will consist of examples of non-trivial experiments, showing how these can be packed in a Linux machine and reproduced on different machines and operating systems. Demo visitors will also be able to pack and reproduce their own experiments.},
  isbn = {978-1-4503-3531-7},
  langid = {english},
  keywords = {computational environment,computational reproducibility,reprozip},
  file = {/Users/r.d.wit/Zotero/storage/76LST83F/Chirigati e.a. - 2016 - ReproZip Computational Reproducibility With Ease.pdf}
}

@inproceedings{clarkBCEBerkeleyCommon2014,
  title = {{{BCE}}: {{Berkeley}}'s {{Common Scientific Compute Environment}} for {{Research}} and {{Education}}},
  shorttitle = {{{BCE}}},
  booktitle = {Python in {{Science Conference}}},
  author = {Clark, Dav and Culich, Aaron and Hamlin, Brian and Lovett, Ryan},
  year = {2014},
  pages = {5--12},
  address = {{Austin, Texas}},
  doi = {10.25080/Majora-14bd3278-002},
  urldate = {2022-05-09},
  abstract = {There are numerous barriers to the use of scientific computing toolsets. These barriers are becoming more apparent as we increasingly see mixing of different academic backgrounds, and compute ranging from laptops to cloud platforms. Members of the UC Berkeley D-Lab, Statistical Computing Facility (SCF), and Berkeley Research Computing (BRC) support such use-cases, and have developed strategies that reduce the pain points that arise. We begin by describing the variety of concrete training and research use-cases in which our strategy might increase accessibility, productivity, reuse, and reproducibility. We then introduce available tools for the ``recipe-based'' creation of compute environments, attempting to demystify and provide a framework for thinking about DevOps (along with explaining what ``DevOps'' means!). As a counterpoint to novel DevOps tools, we'll also examine the success of OSGeo-Live [OSGL] \textendash{} a project that has managed to obtain and manage developer contributions for a large number of geospatial projects. This is enabled through the use of commonly known skills like shell scripting, and is a model of complexity that can be managed without these more recent DevOps tools. Given our evaluation of a variety of technologies and use-cases, we present our current strategy for constructing the Berkeley Common Environment [BCE], along with general recommendations for building environments for your own use-cases.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/EB5Y6YIK/Clark e.a. - 2014 - BCE Berkeley's Common Scientific Compute Environm.pdf}
}

@article{clarkMicropublicationsSemanticModel2014,
  title = {Micropublications: A Semantic Model for Claims, Evidence, Arguments and Annotations in Biomedical Communications},
  shorttitle = {Micropublications},
  author = {Clark, Tim and Ciccarese, Paolo N and Goble, Carole A},
  year = {2014},
  journal = {Journal of Biomedical Semantics},
  volume = {5},
  number = {1},
  pages = {28},
  issn = {2041-1480},
  doi = {10.1186/2041-1480-5-28},
  urldate = {2022-05-05},
  abstract = {Background: Scientific publications are documentary representations of defeasible arguments, supported by data and repeatable methods. They are the essential mediating artifacts in the ecosystem of scientific communications. The institutional ``goal'' of science is publishing results. The linear document publication format, dating from 1665, has survived transition to the Web. Intractable publication volumes; the difficulty of verifying evidence; and observed problems in evidence and citation chains suggest a need for a web-friendly and machine-tractable model of scientific publications. This model should support: digital summarization, evidence examination, challenge, verification and remix, and incremental adoption. Such a model must be capable of expressing a broad spectrum of representational complexity, ranging from minimal to maximal forms. Results: The micropublications semantic model of scientific argument and evidence provides these features. Micropublications support natural language statements; data; methods and materials specifications; discussion and commentary; challenge and disagreement; as well as allowing many kinds of statement formalization. The minimal form of a micropublication is a statement with its attribution. The maximal form is a statement with its complete supporting argument, consisting of all relevant evidence, interpretations, discussion and challenges brought forward in support of or opposition to it. Micropublications may be formalized and serialized in multiple ways, including in RDF. They may be added to publications as stand-off metadata. An OWL 2 vocabulary for micropublications is available at http://purl.org/mp. A discussion of this vocabulary along with RDF examples from the case studies, appears as OWL Vocabulary and RDF Examples in Additional file 1. Conclusion: Micropublications, because they model evidence and allow qualified, nuanced assertions, can play essential roles in the scientific communications ecosystem in places where simpler, formalized and purely statement-based models, such as the nanopublications model, will not be sufficient. At the same time they will add significant value to, and are intentionally compatible with, statement-based formalizations. We suggest that micropublications, generated by useful software tools supporting such activities as writing, editing, reviewing, and discussion, will be of great value in improving the quality and tractability of biomedical communications.},
  langid = {english},
  keywords = {micropublications,ontology,scientific reasoning},
  file = {/Users/r.d.wit/Zotero/storage/2MEAJKVM/Clark e.a. - 2014 - Micropublications a semantic model for claims, ev.pdf}
}

@article{collbergMeasuringReproducibilityComputer2013,
  title = {Measuring {{Reproducibility}} in {{Computer Systems Research}}},
  author = {Collberg, Christian and Proebsting, Todd and Moraila, Gina and Shankaran, Akash and Shi, Zuoming and Warren, Alex M.},
  year = {2013},
  urldate = {2022-08-20}
}

@book{committeeonreproducibilityandreplicabilityinscienceReproducibilityReplicabilityScience2019,
  title = {Reproducibility and {{Replicability}} in {{Science}}},
  author = {{Committee on Reproducibility and Replicability in Science} and {Board on Behavioral, Cognitive, and Sensory Sciences} and {Committee on National Statistics} and {Division of Behavioral and Social Sciences and Education} and {Nuclear and Radiation Studies Board} and {Division on Earth and Life Studies} and {Board on Mathematical Sciences and Analytics} and {Committee on Applied and Theoretical Statistics} and {Division on Engineering and Physical Sciences} and {Board on Research Data and Information} and {Committee on Science, Engineering, Medicine, and Public Policy} and {Policy and Global Affairs} and {National Academies of Sciences, Engineering, and Medicine}},
  year = {2019},
  month = sep,
  pages = {25303},
  publisher = {{National Academies Press}},
  address = {{Washington, D.C.}},
  doi = {10.17226/25303},
  urldate = {2022-07-27},
  isbn = {978-0-309-48616-3},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/HQJWLVXL/Committee on Reproducibility and Replicability in Science e.a. - 2019 - Reproducibility and Replicability in Science.pdf}
}

@misc{crusoeCommonworkflowlanguageCwltool202305131557342023,
  title = {Common-Workflow-Language/Cwltool: 3.1.20230513155734},
  shorttitle = {Common-Workflow-Language/Cwltool},
  author = {Crusoe, Michael R. and Khan, Farah Zaib and Amstutz, Peter and {Soiland-Reyes}, Stian and Manvendra Singh and Kumar, Kapil and Safont, Pau Ruiz and Chilton, John and Hickman, Thomas and Boysha and Tanjo, Tomoya and Nash, Rupert and Hannon, Kevin and Ash and Kotliar, Michael and Chapman, Brad and Kartashov, Andrey and Carrasco, Guillermo and Leehr, Dan and Nebojsa Tijanic and Randall, Joshua C. and Boland, Miguel and Bogdang989 and McCallum, Chuck and M{\'e}nager, Herv{\'e} and Kinoshita, Bruno P. and Yuen, Denis and Molenaar, Gijs and Porter, James J},
  year = {2023},
  month = may,
  doi = {10.5281/ZENODO.7933141},
  urldate = {2023-05-20},
  abstract = {What's Changed Update ruamel-yaml requirement from \&lt;0.17.22,\&gt;=0.16.0 to \&gt;=0.16.0,\&lt;0.17.27 by @dependabot in https://github.com/common-workflow-language/cwltool/pull/1838 --leave-tmpdir includes the input staging directories by @mr-c in https://github.com/common-workflow-language/cwltool/pull/1674 https://github.com/common-workflow-language/cwltool/pull/1840 Bump mypy from 1.2.0 to 1.3.0 by @dependabot in https://github.com/common-workflow-language/cwltool/pull/1839 {$<$}strong{$>$}Full Changelog{$<$}/strong{$>$}: https://github.com/common-workflow-language/cwltool/compare/3.1.20230425144158...3.1.20230513155734},
  copyright = {Open Access},
  howpublished = {Zenodo},
  url          = {https://doi.org/10.5281/zenodo.7933141}
}



@article{crusoeMethodsIncludedStandardizing2022,
  title = {Methods Included: Standardizing Computational Reuse and Portability with the {{Common Workflow Language}}},
  shorttitle = {Methods Included},
  author = {Crusoe, Michael R. and Abeln, Sanne and Iosup, Alexandru and Amstutz, Peter and Chilton, John and Tijani{\'c}, Neboj{\v s}a and M{\'e}nager, Herv{\'e} and {Soiland-Reyes}, Stian and Gavrilovi{\'c}, Bogdan and Goble, Carole and Community, The CWL},
  year = {2022},
  month = jun,
  journal = {Communications of the ACM},
  volume = {65},
  number = {6},
  pages = {54--63},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3486897},
  urldate = {2022-08-17},
  abstract = {Standardizing computational reuse and portability with the Common Workflow Language.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/GTL7DXNV/Crusoe e.a. - 2022 - Methods included standardizing computational reus.pdf}
}

@TechReport{cyganiakRDFConceptsAbstract2014,
  author      = "Richard Cyganiak and David Wood and Markus Lanthaler",
  title       = "{RDF} 1.1 Concepts and Abstract Syntax",
  month       = feb,
  url        = "https://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/",
  year        = "2014",
  bibsource   = "https://w2.syronex.com/jmr/w3c-biblio",
  type        = "{W3C} Recommendation",
  institution = "W3C",
}

@article{DataCitationNeeded2019,
  title = {Data Citation Needed},
  year = {2019},
  month = dec,
  journal = {Scientific Data},
  volume = {6},
  number = {1},
  pages = {27, s41597-019-0026-5},
  issn = {2052-4463},
  doi = {10.1038/s41597-019-0026-5},
  urldate = {2022-04-19},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/6KYGZREI/2019 - Data citation needed.pdf}
}

@article{datacitationsynthesisgroupJointDeclarationData2014,
  title = {Joint {{Declaration}} of {{Data Citation Principles}}},
  author = {{Data Citation Synthesis Group}},
  editor = {Martone, M},
  year = {2014},
  journal = {San Diego CA: FORCE11},
  doi = {10.25490/a97f-egyk}
}

@article{datacitemetadataworkinggroupDataCiteMetadataSchema2021,
  title = {{{DataCite Metadata Schema Documentation}} for the {{Publication}} and {{Citation}} of {{Research Data}} and {{Other Research Outputs}} v4.4},
  author = {DataCite Metadata Working Group},
  year = {2021},
  pages = {82 pages},
  publisher = {{DataCite}},
  doi = {10.14454/3W3Z-SA82},
  urldate = {2022-05-09},
  collaborator = {{de Smaele, Madeleine} and Bernal Mart{\'i}nez, Isabel and Dasler, Robin and Ashton, Jan and Roy, Sophie and Fenner, Martin and Chiloane, Leo and Burger, Marleen and Yahia, Mohamed and Zolly, Lisa and Habermann, Ted and Raugh, Anne and Ilik, Violeta and Foulger, Samantha},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/8GNPEW2E/DataCite Metadata Working Group - 2021 - DataCite Metadata Schema Documentation for the Pub.pdf}
}

@article{daveigaleprevostBioContainersOpensourceCommunitydriven2017,
  title = {{{BioContainers}}: An Open-Source and Community-Driven Framework for Software Standardization},
  shorttitle = {{{BioContainers}}},
  author = {{da Veiga Leprevost}, Felipe and Gr{\"u}ning, Bj{\"o}rn A and Alves Aflitos, Saulo and R{\"o}st, Hannes L and Uszkoreit, Julian and Barsnes, Harald and Vaudel, Marc and Moreno, Pablo and Gatto, Laurent and Weber, Jonas and Bai, Mingze and Jimenez, Rafael C and Sachsenberg, Timo and Pfeuffer, Julianus and Vera Alvarez, Roberto and Griss, Johannes and Nesvizhskii, Alexey I and {Perez-Riverol}, Yasset},
  editor = {Valencia, Alfonso},
  year = {2017},
  month = aug,
  journal = {Bioinformatics},
  volume = {33},
  number = {16},
  pages = {2580--2582},
  issn = {1367-4803, 1460-2059},
  doi = {10.1093/bioinformatics/btx192},
  urldate = {2022-05-06},
  langid = {english},
  keywords = {BioContainers,reproducibility},
  file = {/Users/r.d.wit/Zotero/storage/RGKICMDB/da Veiga Leprevost e.a. - 2017 - BioContainers an open-source and community-driven.pdf}
}

@book{ducharmeLearningSPARQL2013,
  title = {Learning {{SPARQL}}},
  shorttitle = {Learning {{SPARQL}}},
  author = {DuCharme, Bob},
  year = {2013},
  edition = {Second},
  publisher = {{O'Reilly Media}},
  isbn = {978-1-4493-7143-2},
  file = {/Users/r.d.wit/Zotero/storage/RJ4TXMG2/DuCharme_learningSPARQL.epub}
}

@article{dunbarSAbDabStructuralAntibody2014,
  title = {{{SAbDab}}: The Structural Antibody Database},
  shorttitle = {{{SAbDab}}},
  author = {Dunbar, James and Krawczyk, Konrad and Leem, Jinwoo and Baker, Terry and Fuchs, Angelika and Georges, Guy and Shi, Jiye and Deane, Charlotte M.},
  year = {2014},
  month = jan,
  journal = {Nucleic Acids Research},
  volume = {42},
  number = {D1},
  pages = {D1140-D1146},
  issn = {0305-1048, 1362-4962},
  doi = {10.1093/nar/gkt1043},
  urldate = {2022-08-08},
  abstract = {Structural antibody database (SAbDab; http://opig. stats.ox.ac.uk/webapps/sabdab) is an online resource containing all the publicly available antibody structures annotated and presented in a consistent fashion. The data are annotated with several properties including experimental information, gene details, correct heavy and light chain pairings, antigen details and, where available, antibody\textendash antigen binding affinity. The user can select structures, according to these attributes as well as structural properties such as complementarity determining region loop conformation and variable domain orientation. Individual structures, datasets and the complete database can be downloaded.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/WW76ZCKN/Dunbar e.a. - 2014 - SAbDab the structural antibody database.pdf}
}

@article{fennerDataCitationRoadmap2019,
  title = {A Data Citation Roadmap for Scholarly Data Repositories},
  author = {Fenner, Martin and Crosas, Merc{\`e} and Grethe, Jeffrey S. and Kennedy, David and Hermjakob, Henning and {Rocca-Serra}, Phillippe and Durand, Gustavo and Berjon, Robin and Karcher, Sebastian and Martone, Maryann and Clark, Tim},
  year = {2019},
  month = dec,
  journal = {Scientific Data},
  volume = {6},
  number = {1},
  pages = {28},
  issn = {2052-4463},
  doi = {10.1038/s41597-019-0031-8},
  urldate = {2022-04-19},
  langid = {english},
  keywords = {data citation},
  file = {/Users/r.d.wit/Zotero/storage/LZH2IDNQ/Fenner e.a. - 2019 - A data citation roadmap for scholarly data reposit.pdf}
}

@article{ferreiradasilvaCharacterizationWorkflowManagement2017,
  title = {A Characterization of Workflow Management Systems for Extreme-Scale Applications},
  author = {{Ferreira da Silva}, Rafael and Filgueira, Rosa and Pietri, Ilia and Jiang, Ming and Sakellariou, Rizos and Deelman, Ewa},
  year = {2017},
  month = oct,
  journal = {Future Generation Computer Systems},
  volume = {75},
  pages = {228--238},
  issn = {0167739X},
  doi = {10.1016/j.future.2017.02.026},
  urldate = {2023-03-01},
  langid = {english},
  keywords = {CWL Conference 2023},
  file = {/Users/r.d.wit/Zotero/storage/HB9A3CZH/Ferreira da Silva et al. - 2017 - A characterization of workflow management systems .pdf}
}

@article{GAIAXTechnicalArchitecture,
  title = {{{GAIA-X}}: {{Technical Architecture}}},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/K24KLZXJ/GAIA-X Technical Architecture.pdf}
}

@article{gaignardFindableReusableWorkflow2020,
  title = {Findable and Reusable Workflow Data Products: {{A}} Genomic Workflow Case Study},
  shorttitle = {Findable and Reusable Workflow Data Products},
  author = {Gaignard, Alban and {Skaf-Molli}, Hala and Belhajjame, Khalid},
  editor = {Kauppinen, Tomi and Garijo, Daniel and Villanueva, Natalia and Garijo, Daniel and {Villanueva-Rosales}, Natalia and Kauppinen, Tomi},
  year = {2020},
  month = aug,
  journal = {Semantic Web},
  volume = {11},
  number = {5},
  pages = {751--763},
  issn = {22104968, 15700844},
  doi = {10.3233/SW-200374},
  urldate = {2022-05-05},
  abstract = {While workflow systems have improved the repeatability of scientific experiments, the value of the processed (intermediate) data have been overlooked so far. In this paper, we argue that the intermediate data products of workflow executions should be seen as first-class objects that need to be curated and published. Not only will this be exploited to save time and resources needed when re-executing workflows, but more importantly, it will improve the reuse of data products by the same or peer scientists in the context of new hypotheses and experiments. To assist curator in annotating (intermediate) workflow data, we exploit in this work multiple sources of information, namely: (i) the provenance information captured by the workflow system, and (ii) domain annotations that are provided by tools registries, such as Bio.Tools. Furthermore, we show, on a concrete bioinformatics scenario, how summarising techniques can be used to reduce the machine-generated provenance information of such data products into concise human- and machine-readable annotations.},
  langid = {english},
  keywords = {FAIR,FRESH,ontologies,provenance analytics,SPARQL,use case workflow},
  file = {/Users/r.d.wit/Zotero/storage/WA3XKN5K/Gaignard e.a. - 2020 - Findable and reusable workflow data products A ge.pdf}
}

@article{garijoArtificialIntelligenceBuzzword,
  title = {Artificial {{Intelligence Buzzword Explained}}: {{Scientific}} Workflows},
  author = {Garijo, Daniel},
  pages = {4},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/LN8HG9TG/Garijo - Artificial Intelligence Buzzword Explained Scient.pdf}
}

@article{garijoCommonMotifsScientific2014,
  title = {Common Motifs in Scientific Workflows: {{An}} Empirical Analysis},
  shorttitle = {Common Motifs in Scientific Workflows},
  author = {Garijo, Daniel and Alper, Pinar and Belhajjame, Khalid and Corcho, Oscar and Gil, Yolanda and Goble, Carole},
  year = {2014},
  month = jul,
  journal = {Future Generation Computer Systems},
  volume = {36},
  pages = {338--351},
  issn = {0167739X},
  doi = {10.1016/j.future.2013.09.018},
  urldate = {2022-03-03},
  abstract = {Workflow technology continues to play an important role as a means for specifying and enacting computational experiments in modern science. Reusing and re-purposing workflows allow scientists to do new experiments faster, since the workflows capture useful expertise from others. As workflow libraries grow, scientists face the challenge of finding workflows appropriate for their task, understanding what each workflow does, and reusing relevant portions of a given workflow. We believe that workflows would be easier to understand and reuse if high-level views (abstractions) of their activities were available in workflow libraries. As a first step towards obtaining these abstractions, we report in this paper on the results of a manual analysis performed over a set of real-world scientific workflows from Taverna, Wings, Galaxy and Vistrails. Our analysis has resulted in a set of scientific workflow motifs that outline (i) the kinds of data-intensive activities that are observed in workflows (Data-Operation motifs), and (ii) the different manners in which activities are implemented within workflows (Workflow-Oriented motifs). These motifs are helpful to identify the functionality of the steps in a given workflow, to develop best practices for workflow design, and to develop approaches for automated generation of workflow abstractions.},
  langid = {english},
  keywords = {workflows},
  file = {/Users/r.d.wit/Zotero/storage/TRUVNM94/Garijo e.a. - 2014 - Common motifs in scientific workflows An empirica.pdf}
}

@inproceedings{garijoDetectingCommonScientific2013,
  title = {Detecting Common Scientific Workflow Fragments Using Templates and Execution Provenance},
  booktitle = {Proceedings of the Seventh International Conference on {{Knowledge}} Capture},
  author = {Garijo, Daniel and Corcho, Oscar and Gil, Yolanda},
  year = {2013},
  month = jun,
  pages = {33--40},
  publisher = {{ACM}},
  address = {{Banff Canada}},
  doi = {10.1145/2479832.2479848},
  urldate = {2022-04-01},
  abstract = {Provenance plays a major role when understanding and reusing the methods applied in a scientific experiment, as it provides a record of inputs, the processes carried out and the use and generation of intermediate and final results. In the specific case of in-silico scientific experiments, a large variety of scientific workflow systems (e.g., Wings, Taverna, Galaxy, Vistrails) have been created to support scientists. All of these systems produce some sort of provenance about the executions of the workflows that encode scientific experiments. However, provenance is normally recorded at a very low level of detail, which complicates the understanding of what happened during execution. In this paper we propose an approach to automatically obtain abstractions from low-level provenance data by finding common workflow fragments on workflow execution provenance and relating them to templates. We have tested our approach with a dataset of workflows published by the Wings workflow system. Our results show that by using these kinds of abstractions we can highlight the most common abstract methods used in the executions of a repository, relating different runs and workflow templates with each other.},
  isbn = {978-1-4503-2102-0},
  langid = {english},
  keywords = {workflows},
  file = {/Users/r.d.wit/Zotero/storage/VSR34GMT/Garijo e.a. - 2013 - Detecting common scientific workflow fragments usi.pdf}
}

@article{garijoQuantifyingReproducibilityComputational2013,
  title = {Quantifying {{Reproducibility}} in {{Computational Biology}}: {{The Case}} of the {{Tuberculosis Drugome}}},
  shorttitle = {Quantifying {{Reproducibility}} in {{Computational Biology}}},
  author = {Garijo, Daniel and Kinnings, Sarah and Xie, Li and Xie, Lei and Zhang, Yinliang and Bourne, Philip E. and Gil, Yolanda},
  editor = {Ouzounis, Christos A.},
  year = {2013},
  month = nov,
  journal = {PLoS ONE},
  volume = {8},
  number = {11},
  pages = {e80278},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0080278},
  urldate = {2022-06-02},
  abstract = {How easy is it to reproduce the results found in a typical computational biology paper? Either through experience or intuition the reader will already know that the answer is with difficulty or not at all. In this paper we attempt to quantify this difficulty by reproducing a previously published paper for different classes of users (ranging from users with little expertise to domain experts) and suggest ways in which the situation might be improved. Quantification is achieved by estimating the time required to reproduce each of the steps in the method described in the original paper and make them part of an explicit workflow that reproduces the original results. Reproducing the method took several months of effort, and required using new versions and new software that posed challenges to reconstructing and validating the results. The quantification leads to ``reproducibility maps'' that reveal that novice researchers would only be able to reproduce a few of the steps in the method, and that only expert researchers with advance knowledge of the domain would be able to reproduce the method in its entirety. The workflow itself is published as an online resource together with supporting software and data. The paper concludes with a brief discussion of the complexities of requiring reproducibility in terms of cost versus benefit, and a desiderata with our observations and guidelines for improving reproducibility. This has implications not only in reproducing the work of others from published papers, but reproducing work from one's own laboratory.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/GLLCVUEL/Garijo e.a. - 2013 - Quantifying Reproducibility in Computational Biolo.pdf}
}

@inproceedings{gilAutomatingDataNarratives2017,
  title = {Towards {{Automating Data Narratives}}},
  booktitle = {Proceedings of the 22nd {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Gil, Yolanda and Garijo, Daniel},
  year = {2017},
  month = mar,
  pages = {565--576},
  publisher = {{ACM}},
  address = {{Limassol Cyprus}},
  doi = {10.1145/3025171.3025193},
  urldate = {2022-05-05},
  abstract = {We propose a new area of research on automating data narratives. Data narratives are containers of information about computationally generated research findings. They have three major components: 1) A record of events, that describe a new result through a workflow and/or provenance of all the computations executed; 2) Persistent entries for key entities involved for data, software versions, and workflows; 3) A set of narrative accounts that are automatically generated humanconsumable renderings of the record and entities and can be included in a paper. Different narrative accounts can be used for different audiences with different content and details, based on the level of interest or expertise of the reader. Data narratives can make science more transparent and reproducible, because they ensure that the text description of the computational experiment reflects with high fidelity what was actually done. Data narratives can be incorporated in papers, either in the methods section or as supplementary materials. We introduce DANA, a prototype that illustrates how to generate data narratives automatically, and describe the information it uses from the computational records. We also present a formative evaluation of our approach and discuss potential uses of automated data narratives.},
  isbn = {978-1-4503-4348-0},
  langid = {english},
  keywords = {data narratives,discussion,introduction,narrative accounts,reproducibility problems},
  file = {/Users/r.d.wit/Zotero/storage/KWNUQFP5/Gil en Garijo - 2017 - Towards Automating Data Narratives.pdf}
}

@article{gilGeosciencePaperFuture2016,
  title = {Toward the {{Geoscience Paper}} of the {{Future}}: {{Best}} Practices for Documenting and Sharing Research from Data to Software to Provenance},
  shorttitle = {Toward the {{Geoscience Paper}} of the {{Future}}},
  author = {Gil, Yolanda and David, C{\'e}dric H. and Demir, Ibrahim and Essawy, Bakinam T. and Fulweiler, Robinson W. and Goodall, Jonathan L. and Karlstrom, Leif and Lee, Huikyo and Mills, Heath J. and Oh, Ji-Hyun and Pierce, Suzanne A. and Pope, Allen and Tzeng, Mimi W. and Villamizar, Sandra R. and Yu, Xuan},
  year = {2016},
  month = oct,
  journal = {Earth and Space Science},
  volume = {3},
  number = {10},
  pages = {388--415},
  issn = {2333-5084, 2333-5084},
  doi = {10.1002/2015EA000136},
  urldate = {2022-08-05},
  abstract = {Geoscientists now live in a world rich with digital data and methods, and their computational research cannot be fully captured in traditional publications. The Geoscience Paper of the Future (GPF) presents an approach to fully document, share, and cite all their research products including data, software, and computational provenance. This article proposes best practices for GPF authors to make data, software, and methods openly accessible, citable, and well documented. The publication of digital objects empowers scientists to manage their research products as valuable scientific assets in an open and transparent way that enables broader access by other scientists, students, decision makers, and the public. Improving documentation and dissemination of research will accelerate the pace of scientific discovery by improving the ability of others to build upon published work.},
  langid = {english},
  keywords = {geoscience,provenance,recommendations,reproducibility,scientific writing},
  file = {/Users/r.d.wit/Zotero/storage/F45UN9DU/Gil e.a. - 2016 - Toward the Geoscience Paper of the Future Best pr.pdf}
}

@book{glavicProvenanceAnnotationData2021,
  title = {Provenance and {{Annotation}} of {{Data}} and {{Processes}}: 8th and 9th {{International Provenance}} and {{Annotation Workshop}}, {{IPAW}} 2020 + {{IPAW}} 2021, {{Virtual Event}}, {{July}} 19\textendash 22, 2021, {{Proceedings}}},
  shorttitle = {Provenance and {{Annotation}} of {{Data}} and {{Processes}}},
  editor = {Glavic, Boris and Braganholo, Vanessa and Koop, David},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12839},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-80960-7},
  urldate = {2022-03-20},
  isbn = {978-3-030-80959-1 978-3-030-80960-7},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/9F3E5KVV/Glavic e.a. - 2021 - Provenance and Annotation of Data and Processes 8.pdf}
}

@article{gobleFAIRComputationalWorkflows2020,
  title = {{{FAIR Computational Workflows}}},
  author = {Goble, Carole and {Cohen-Boulakia}, Sarah and {Soiland-Reyes}, Stian and Garijo, Daniel and Gil, Yolanda and Crusoe, Michael R. and Peters, Kristian and Schober, Daniel},
  year = {2020},
  month = jan,
  journal = {Data Intelligence},
  volume = {2},
  number = {1-2},
  pages = {108--121},
  issn = {2641-435X},
  doi = {10.1162/dint_a_00033},
  urldate = {2022-04-25},
  abstract = {Computational workflows describe the complex multi-step methods that are used for data collection, data preparation, analytics, predictive modelling, and simulation that lead to new data products. They can inherently contribute to the FAIR data principles: by processing data according to established metadata; by creating metadata themselves during the processing of data; and by tracking and recording data provenance. These properties aid data quality assessment and contribute to secondary data usage. Moreover, workflows are digital objects in their own right. This paper argues that FAIR principles for workflows need to address their specific nature in terms of their composition of executable software steps, their provenance, and their development.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/Z23VTQIJ/Goble e.a. - 2020 - FAIR Computational Workflows.pdf}
}

@inproceedings{grayPotatoSaladProtein2017,
 title = "Bioschemas: From Potato Salad to Protein Annotation",
abstract = "The life sciences have a wealth of data resources with a wide range of overlapping content. Key repositories, such as UniProt for protein data or Entrez Gene for gene data, are well known and their content easily discovered through search engines. However, there is a long-tail of bespoke datasets with important content that are not so prominent in search results. Building on the success of Schema.org for making a wide range of structured web content more discoverable and interpretable, e.g. food recipes, the Bioschemas community aim to make life sciences datasets more findable by encouraging data providers to embed Schema.org markup in their resources.",
keywords = "schema.org, metadata, dataset descriptions, data discovery",
author = "Gray, {Alasdair J G} and Carole Goble and Jimenez, {Rafael C.}",
year = "2017",
month = oct,
day = "22",
language = "English",
series = "CEUR workshop proceedings",
publisher = "RWTH Aachen University",
editor = "Nadeschda Nikitina and Dezhao Song and Fokoue, {Achille } and Haase, { Peter}",
booktitle = "ISWC 2017 Posters \& Demonstrations and Industry Tracks",
address = "Germany",
edition = "urn:nbn:de:0074-1963-7",
note = "The 16th International Semantic Web Conference 2017, ISWC 2017 ; Conference date: 21-10-2018 Through 25-10-2018",
url = "https://iswc2017.semanticweb.org/calls/call-for-research-track-papers/",
}

@book{grossmannAdvancesConceptualModeling2020,
  title = {Advances in {{Conceptual Modeling}}: {{ER}} 2020 {{Workshops CMAI}}, {{CMLS}}, {{CMOMM4FAIR}}, {{CoMoNoS}}, {{EmpER}}, {{Vienna}}, {{Austria}}, {{November}} 3\textendash 6, 2020, {{Proceedings}}},
  shorttitle = {Advances in {{Conceptual Modeling}}},
  editor = {Grossmann, Georg and Ram, Sudha},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12584},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-65847-2},
  urldate = {2023-02-10},
  isbn = {978-3-030-65846-5 978-3-030-65847-2},
  langid = {english},
  keywords = {FAIR implementation profiles (FIP)},
  file = {/Users/r.d.wit/Zotero/storage/W3WVE9Y7/Grossmann and Ram - 2020 - Advances in Conceptual Modeling ER 2020 Workshops.pdf}
}

@article{grothFAIRDataReuse2020,
  title = {{{FAIR Data Reuse}} \textendash{} the {{Path}} through {{Data Citation}}},
  author = {Groth, Paul and Cousijn, Helena and Clark, Tim and Goble, Carole},
  year = {2020},
  month = jan,
  journal = {Data Intelligence},
  volume = {2},
  number = {1-2},
  pages = {78--86},
  issn = {2641-435X},
  doi = {10.1162/dint_a_00030},
  urldate = {2022-05-15},
  abstract = {One of the key goals of the FAIR guiding principles is defined by its final principle \textendash{} to optimize data sets for reuse by both humans and machines. To do so, data providers need to implement and support consistent machine readable metadata to describe their data sets. This can seem like a daunting task for data providers, whether it is determining what level of detail should be provided in the provenance metadata or figuring out what common shared vocabularies should be used. Additionally, for existing data sets it is often unclear what steps should be taken to enable maximal, appropriate reuse. Data citation already plays an important role in making data findable and accessible, providing persistent and unique identifiers plus metadata on over 16 million data sets. In this paper, we discuss how data citation and its underlying infrastructures, in particular associated metadata, provide an important pathway for enabling FAIR data reuse.},
  langid = {english},
  keywords = {data citation,DataCite,FAIR,Wikidata},
  file = {/Users/r.d.wit/Zotero/storage/92F4NMPD/Groth e.a. - 2020 - FAIR Data Reuse – the Path through Data Citation.pdf}
}

@article{gruningPracticalComputationalReproducibility2018,
  title = {Practical {{Computational Reproducibility}} in the {{Life Sciences}}},
  author = {Gr{\"u}ning, Bj{\"o}rn and Chilton, John and K{\"o}ster, Johannes and Dale, Ryan and Soranzo, Nicola and {van den Beek}, Marius and Goecks, Jeremy and Backofen, Rolf and Nekrutenko, Anton and Taylor, James},
  year = {2018},
  month = jun,
  journal = {Cell Systems},
  volume = {6},
  number = {6},
  pages = {631--635},
  issn = {24054712},
  doi = {10.1016/j.cels.2018.03.014},
  urldate = {2022-04-01},
  langid = {english},
  keywords = {bioconda,conda,Docker,reproducibility,VM},
  file = {/Users/r.d.wit/Zotero/storage/8PWB5FHF/Grüning e.a. - 2018 - Practical Computational Reproducibility in the Lif.pdf}
}

@article{grykWorkflowsProvenanceInformation2017,
  title = {Workflows and {{Provenance}}: {{Toward Information Science Solutions}} for the {{Natural Sciences}}},
  shorttitle = {Workflows and {{Provenance}}},
  author = {Gryk, Michael R. and Lud{\"a}scher, Bertram},
  year = {2017},
  journal = {Library Trends},
  volume = {65},
  number = {4},
  pages = {555--562},
  issn = {1559-0682},
  doi = {10.1353/lib.2017.0018},
  urldate = {2022-03-17},
  abstract = {The era of big data and ubiquitous computation has brought with it concerns about ensuring reproducibility in this new research environment. It is easy to assume that computational methods self-document by their very nature of being exact, deterministic processes. However, similar to laboratory experiments, ensuring reproducibility in the computational realm requires the documentation of both the protocols used (workflows), as well as a detailed description of the computational environment: algorithms, implementations, software environments, and the data ingested and execution logs of the computation. These two aspects of computational reproducibility (workflows and execution details) are discussed within the context of biomolecular Nuclear Magnetic Resonance spectroscopy (bioNMR), as well as the PRIMAD model for computational reproducibility.},
  langid = {english},
  keywords = {prospective provenance,provenance,retrospective provenance,workflows},
  file = {/Users/r.d.wit/Zotero/storage/DEATZG5R/Gryk en Ludäscher - 2017 - Workflows and Provenance Toward Information Scien.pdf}
}

@article{guhaBigDataMakes2015,
    author = {Guha, R. V. and Brickley, Dan and Macbeth, Steve},
    title = {Schema.org: evolution of structured data on the web},
    year = {2016},
    issue_date = {February 2016},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {59},
    number = {2},
    issn = {0001-0782},
    url = {https://doi.org/10.1145/2844544},
    doi = {10.1145/2844544},
    abstract = {Big data makes common schemas even more necessary.},
    journal = {Commun. ACM},
    month = {jan},
    pages = {44–51},
    numpages = {8}
    }


@inproceedings{hashamScientificWorkflowRepeatability2014,
  title = {Scientific {{Workflow Repeatability}} through {{Cloud-Aware Provenance}}},
  booktitle = {2014 {{IEEE}}/{{ACM}} 7th {{International Conference}} on {{Utility}} and {{Cloud Computing}}},
  author = {Hasham, Khawar and Munir, Kamran and Shamdasani, Jetendr and McClatchey, Richard},
  year = {2014},
  month = dec,
  pages = {951--956},
  publisher = {{IEEE}},
  address = {{London, United Kingdom}},
  doi = {10.1109/UCC.2014.155},
  urldate = {2022-05-15},
  abstract = {The transformations, analyses and interpretations of data in scientific workflows are vital for the repeatability and reliability of scientific workflows. This provenance of scientific workflows has been effectively carried out in Grid based scientific workflow systems. However, recent adoption of Cloudbased scientific workflows present an opportunity to investigate the suitability of existing approaches or propose new approaches to collect provenance information from the Cloud and to utilize it for workflow repeatability in the Cloud infrastructure. The dynamic nature of the Cloud in comparison to the Grid makes it difficult because resources are provisioned on-demand unlike the Grid. This paper presents a novel approach that can assist in mitigating this challenge. This approach can collect Cloud infrastructure information along with workflow provenance and can establish a mapping between them. This mapping is later used to re-provision resources on the Cloud. The repeatability of the workflow execution is performed by: (a) capturing the Cloud infrastructure information (virtual machine configuration) along with the workflow provenance, and (b) reprovisioning the similar resources on the Cloud and reexecuting the workflow on them. The evaluation of an initial prototype suggests that the proposed approach is feasible and can be investigated further.},
  isbn = {978-1-4799-7881-6},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/876NQSXY/Hasham e.a. - 2014 - Scientific Workflow Repeatability through Cloud-Aw.pdf}
}

@article{hasselbringFAIRResearchData2020,
  title = {From {{FAIR}} Research Data toward {{FAIR}} and Open Research Software},
  author = {Hasselbring, Wilhelm and Carr, Leslie and Hettrick, Simon and Packer, Heather and Tiropanis, Thanassis},
  year = {2020},
  month = feb,
  journal = {it - Information Technology},
  volume = {62},
  number = {1},
  pages = {39--47},
  issn = {2196-7032, 1611-2776},
  doi = {10.1515/itit-2019-0040},
  urldate = {2022-04-25},
  abstract = {The Open Science agenda holds that science advances faster when we can build on existing results. Therefore, research data must be FAIR (Findable, Accessible, Interoperable, and Reusable) in order to advance the findability, reproducibility and reuse of research results. Besides the research data, all the processing steps on these data \textendash{} as basis of scientific publications \textendash{} have to be available, too.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/SC98E6N7/Hasselbring e.a. - 2020 - From FAIR research data toward FAIR and open resea.pdf}
}

@misc{hattonFullComputationalReproducibility2016,
  title = {Full {{Computational Reproducibility}} in {{Biological Science}}: {{Methods}}, {{Software}} and a {{Case Study}} in {{Protein Biology}}},
  shorttitle = {Full {{Computational Reproducibility}} in {{Biological Science}}},
  author = {Hatton, Les and Warr, Gregory},
  year = {2016},
  month = aug,
  number = {arXiv:1608.06897},
  eprint = {1608.06897},
  primaryclass = {q-bio},
  publisher = {{arXiv}},
  urldate = {2022-06-08},
  abstract = {Independent computational reproducibility of scientific results is rapidly becoming of pivotal importance in scientific progress as computation itself plays a more and more central role in so many branches of science. Historically, reproducibility has followed the familiar Popperian [38] model whereby theory cannot be verified by scientific testing, it can only be falsified. Ultimately, this implies that if an experiment cannot be reproduced independently to some satisfactory level of precision, its value is essentially unquantifiable; put brutally, it is impossible to determine its scientific value. The burgeoning presence of software in most scientific work adds a new and particularly opaque layer of complexity [29]. In spite of much recent interest in many scientific areas, emphasis remains more on procedures, strictures and discussion [12, 14, 16, 29, 30, 37, 41], reflecting the inexperience of most scientific journals when it comes to software, rather than the details of how computational reproducibility is actually achieved, for which there appear to be relatively few guiding examples [6, 10, 17]. After considering basic principles, here we show how full computational reproducibility can be achieved in practice at every stage using a case study of a multi-gigabyte protein study on the open SwissProt protein database, from data download all the way to individual figure by figure reproduction as an exemplar for general scientific computation.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {computational reproducibility,D.2.9,J.3,Quantitative Biology - Quantitative Methods},
  file = {/Users/r.d.wit/Zotero/storage/NRPQBCHA/Hatton en Warr - 2016 - Full Computational Reproducibility in Biological S.pdf}
}

@article{houSeRenDIPCESequencebasedInterface2021,
  title = {{{SeRenDIP-CE}}: Sequence-Based Interface Prediction for Conformational Epitopes},
  shorttitle = {{{SeRenDIP-CE}}},
  author = {Hou, Qingzhen and Stringer, Bas and Waury, Katharina and Capel, Henriette and Haydarlou, Reza and Xue, Fuzhong and Abeln, Sanne and Heringa, Jaap and Feenstra, K Anton},
  editor = {Xu, Jinbo},
  year = {2021},
  month = oct,
  journal = {Bioinformatics},
  volume = {37},
  number = {20},
  pages = {3421--3427},
  issn = {1367-4803, 1460-2059},
  doi = {10.1093/bioinformatics/btab321},
  urldate = {2022-03-27},
  abstract = {Motivation: Antibodies play an important role in clinical research and biotechnology, with their specificity determined by the interaction with the antigen's epitope region, as a special type of protein\textendash protein interaction (PPI) interface. The ubiquitous availability of sequence data, allows us to predict epitopes from sequence in order to focus time-consuming wet-lab experiments toward the most promising epitope regions. Here, we extend our previously developed sequence-based predictors for homodimer and heterodimer PPI interfaces to predict epitope residues that have the potential to bind an antibody.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/S3NJ2W4X/Hou e.a. - 2021 - SeRenDIP-CE sequence-based interface prediction f.pdf}
}

@article{howisonSoftwareScientificLiterature2016,
  title = {Software in the Scientific Literature: {{Problems}} with Seeing, Finding, and Using Software Mentioned in the Biology Literature},
  shorttitle = {Software in the Scientific Literature},
  author = {Howison, James and Bullard, Julia},
  year = {2016},
  month = sep,
  journal = {Journal of the Association for Information Science and Technology},
  volume = {67},
  number = {9},
  pages = {2137--2155},
  issn = {23301635},
  doi = {10.1002/asi.23538},
  urldate = {2022-04-19},
  langid = {english},
  keywords = {software citation},
  file = {/Users/r.d.wit/Zotero/storage/P674TFR8/Howison en Bullard - 2016 - Software in the scientific literature Problems wi.pdf}
}

@article{huaComputationallydrivenIdentificationAntibody2017,
  title = {Computationally-Driven Identification of Antibody Epitopes},
  author = {Hua, Casey K and Gacerez, Albert T and Sentman, Charles L and Ackerman, Margaret E and Choi, Yoonjoo and {Bailey-Kellogg}, Chris},
  year = {2017},
  month = dec,
  journal = {eLife},
  volume = {6},
  pages = {e29023},
  issn = {2050-084X},
  doi = {10.7554/eLife.29023},
  urldate = {2022-03-20},
  abstract = {Understanding where antibodies recognize antigens can help define mechanisms of action and provide insights into progression of immune responses. We investigate the extent to which information about binding specificity implicitly encoded in amino acid sequence can be leveraged to identify antibody epitopes. In computationally-driven epitope localization, possible antibody\textendash antigen binding modes are modeled, and targeted panels of antigen variants are designed to experimentally test these hypotheses. Prospective application of this approach to two antibodies enabled epitope localization using five or fewer variants per antibody, or alternatively, a six-variant panel for both simultaneously. Retrospective analysis of a variety of antibodies and antigens demonstrated an almost 90\% success rate with an average of three antigen variants, further supporting the observation that the combination of computational modeling and protein design can reveal key determinants of antibody\textendash antigen binding and enable efficient studies of collections of antibodies identified from polyclonal samples or engineered libraries.},
  langid = {english},
  keywords = {epitopes},
  file = {/Users/r.d.wit/Zotero/storage/IAKDAIGB/Hua e.a. - 2017 - Computationally-driven identification of antibody .pdf}
}

@inproceedings{huImprovingDataScientist2020,
  title = {Improving Data Scientist Efficiency with Provenance},
  booktitle = {Proceedings of the {{ACM}}/{{IEEE}} 42nd {{International Conference}} on {{Software Engineering}}},
  author = {Hu, Jingmei and Joung, Jiwon and Jacobs, Maia and Gajos, Krzysztof Z. and Seltzer, Margo I.},
  year = {2020},
  month = jun,
  pages = {1086--1097},
  publisher = {{ACM}},
  address = {{Seoul South Korea}},
  doi = {10.1145/3377811.3380366},
  urldate = {2022-03-29},
  abstract = {Data scientists frequently analyze data by writing scripts. We conducted a contextual inquiry with interdisciplinary researchers, which revealed that parameter tuning is a highly iterative process and that debugging is time-consuming. As analysis scripts evolve and become more complex, analysts have difficulty conceptualizing their workflow. In particular, after editing a script, it becomes difficult to determine precisely which code blocks depend on the edit. Consequently, scientists frequently re-run entire scripts instead of re-running only the necessary parts. We present ProvBuild, a tool that leverages language-level provenance to streamline the debugging process by reducing programmer cognitive load and decreasing subsequent runtimes, leading to an overall reduction in elapsed debugging time. ProvBuild uses provenance to track dependencies in a script. When an analyst debugs a script, ProvBuild generates a simplified script that contains only the information necessary to debug a particular problem. We demonstrate that debugging the simplified script lowers a programmer's cognitive load and permits faster re-execution when testing changes. The combination of reduced cognitive load and shorter runtime reduces the time necessary to debug a script. We quantitatively and qualitatively show that even though ProvBuild introduces overhead during a script's first execution, it is a more efficient way for users to debug and tune complex workflows. ProvBuild demonstrates a novel use of language-level provenance, in which it is used to proactively improve programmer productively rather than merely providing a way to retroactively gain insight into a body of code.},
  isbn = {978-1-4503-7121-6},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/HJYEIQ4S/Hu e.a. - 2020 - Improving data scientist efficiency with provenanc.pdf}
}

@article{huvilaImprovingUsefulnessResearch2022,
  title = {Improving the Usefulness of Research Data with Better Paradata},
  author = {Huvila, Isto},
  year = {2022},
  month = jan,
  journal = {Open Information Science},
  volume = {6},
  number = {1},
  pages = {28--48},
  issn = {2451-1781},
  doi = {10.1515/opis-2022-0129},
  urldate = {2023-02-12},
  abstract = {Considerable investments have been made in Europe and worldwide for developing research data infrastructures. Instead of a general lack of data about data, it has become apparent that a pivotal factor that drastically constrains data use is the absence of contextual knowledge about how data was created and how it has been curated and used. This applies especially to many branches of social science and humanities research, where data is highly heterogeneous, both by its kind (e.g. being qualitative, quantitative, naturalistic, purposefully created) and origins (e.g. being historical/contemporary, from different contexts and geographical places). The problem is that there may be enough metadata (data about data) but there is too little paradata (data on the processes of its creation, curation and use). The aim of this position paper is to draw attention 1) to the need for a better and more systematic understanding and documentation of the contexts of creation, curation and use of research data to make it useful and usable for researchers and other potential users in the future, and 2) to specific obstacles that make the capturing of this particular type of metadata, known as paradata, especially difficult. Failing to understand what information about the creation, curation and use of research data is needed and how to capture enough of that information risks that the currently collected vast amounts of research data become useless in the future.},
  langid = {english},
  keywords = {paradata,TBR},
  file = {/Users/r.d.wit/Zotero/storage/8X6GFETL/Huvila - 2022 - Improving the usefulness of research data with bet.pdf}
}

@incollection{huynhPROVJSONLDJSONLinked2016,
  title = {{{PROV-JSONLD}}: {{A JSON}} and {{Linked Data Representation}} for {{Provenance}}},
  shorttitle = {{{PROV-JSONLD}}},
  booktitle = {Provenance and {{Annotation}} of {{Data}} and {{Processes}}},
  author = {Huynh, Trung Dong and Michaelides, Danius T. and Moreau, Luc},
  editor = {Mattoso, Marta and Glavic, Boris},
  year = {2016},
  volume = {9672},
  pages = {173--177},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-40593-3_15},
  urldate = {2022-05-15},
  abstract = {In this paper, we propose a representation for PROV in JSON-LD, the JSON format for Linked Data, called PROV-JSONLD. As a JSON-based format, this provenance representation can be readily consumed by Web applications currently supporting JSON. As a Linked Data format, at the same time, it also represents provenance data in RDF using the PROV ontology. Hence, it is suitable for usages in both the Web and the Semantic Web.},
  isbn = {978-3-319-40592-6 978-3-319-40593-3},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/TIULESHI/Huynh e.a. - 2016 - PROV-JSONLD A JSON and Linked Data Representation.pdf}
}

@article{iosupAtLargeVisionDesign2019,
  title = {The {{AtLarge Vision}} on the {{Design}} of {{Distributed Systems}} and {{Ecosystems}}},
  author = {Iosup, Alexandru and Versluis, Laurens and Trivedi, Animesh and {van Eyk}, Erwin and Toader, Lucian and {van Beek}, Vincent and Frascaria, Giulia and Musaafir, Ahmed and Talluri, Sacheendra},
  year = {2019},
  month = feb,
  journal = {arXiv:1902.05416 [cs]},
  eprint = {1902.05416},
  primaryclass = {cs},
  urldate = {2022-03-12},
  abstract = {High-quality designs of distributed systems and services are essential for our digital economy and society. Threatening to slow down the stream of working designs, we identify the mounting pressure of scale and complexity of (eco-)system, of ill-defined and wicked problems, and of unclear processes, methods, and tools. We envision design itself as a core research topic in distributed systems, to understand and improve the science and practice of distributed (eco-)system design. Toward this vision, we propose the ATLARGE design framework, accompanied by a set of 8 core design principles. We also propose 10 key challenges, which we hope the community can address in the following 5 years. In our experience so far, the proposed framework and principles are practical, and lead to pragmatic and innovative designs for large-scale distributed systems.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing}},
  file = {/Users/r.d.wit/Zotero/storage/ACC2NMF7/Iosup e.a. - 2019 - The AtLarge Vision on the Design of Distributed Sy.pdf}
}

@article{isonBioToolsRegistry2019,
  title = {The Bio.Tools Registry of Software Tools and Data Resources for the Life Sciences},
  author = {Ison, Jon and Ienasescu, Hans and Chmura, Piotr and Rydza, Emil and M{\'e}nager, Herv{\'e} and Kala{\v s}, Mat{\'u}{\v s} and Schw{\"a}mmle, Veit and Gr{\"u}ning, Bj{\"o}rn and Beard, Niall and Lopez, Rodrigo and Duvaud, Severine and Stockinger, Heinz and Persson, Bengt and Va{\v r}ekov{\'a}, Radka Svobodov{\'a} and Ra{\v c}ek, Tom{\'a}{\v s} and Vondr{\'a}{\v s}ek, Ji{\v r}{\'i} and Peterson, Hedi and Salumets, Ahto and Jonassen, Inge and Hooft, Rob and Nyr{\"o}nen, Tommi and Valencia, Alfonso and Capella, Salvador and Gelp{\'i}, Josep and Zambelli, Federico and Savakis, Babis and Lesko{\v s}ek, Brane and Rapacki, Kristoffer and Blanchet, Christophe and Jimenez, Rafael and Oliveira, Arlindo and Vriend, Gert and Collin, Olivier and {van Helden}, Jacques and L{\o}ngreen, Peter and Brunak, S{\o}ren},
  year = {2019},
  month = dec,
  journal = {Genome Biology},
  volume = {20},
  number = {1},
  pages = {164},
  issn = {1474-760X},
  doi = {10.1186/s13059-019-1772-6},
  urldate = {2022-05-04},
  abstract = {Bioinformaticians and biologists rely increasingly upon workflows for the flexible utilization of the many life science tools that are needed to optimally convert data into knowledge. We outline a pan-European enterprise to provide a catalogue (https://bio.tools) of tools and databases that can be used in these workflows. bio.tools not only lists where to find resources, but also provides a wide variety of practical information.},
  langid = {english},
  keywords = {bio.tools,EDAM,software citation,software registry},
  file = {/Users/r.d.wit/Zotero/storage/EYJFBY8T/Ison e.a. - 2019 - The bio.tools registry of software tools and data .pdf}
}

@article{isonBiotoolsSchemaFormalizedSchema2021,
  title = {{{biotoolsSchema}}: A Formalized Schema for Bioinformatics Software Description},
  shorttitle = {{{biotoolsSchema}}},
  author = {Ison, Jon and Ienasescu, Hans and Rydza, Emil and Chmura, Piotr and Rapacki, Kristoffer and Gaignard, Alban and Schw{\"a}mmle, Veit and {van~Helden}, Jacques and Kala{\v s}, Mat{\'u}{\v s} and M{\'e}nager, Herv{\'e}},
  year = {2021},
  month = jan,
  journal = {GigaScience},
  volume = {10},
  number = {1},
  pages = {giaa157},
  issn = {2047-217X},
  doi = {10.1093/gigascience/giaa157},
  urldate = {2022-05-04},
  abstract = {Background: Life scientists routinely face massive and heterogeneous data analysis tasks and must find and access the most suitable databases or software in a jungle of web-accessible resources. The diversity of information used to describe life-scientific digital resources presents an obstacle to their utilization. Although several standardization efforts are emerging, no information schema has been sufficiently detailed to enable uniform semantic and syntactic description\textemdash and cataloguing\textemdash of bioinformatics resources. Findings: Here we describe biotoolsSchema, a formalized information model that balances the needs of conciseness for rapid adoption against the provision of rich technical information and scientific context. biotoolsSchema results from a series of community-driven workshops and is deployed in the bio.tools registry, providing the scientific community with {$>$}17,000 machine-readable and human-understandable descriptions of software and other digital life-science resources. We compare our approach to related initiatives and provide alignments to foster interoperability and reusability. Conclusions: biotoolsSchema supports the formalized, rigorous, and consistent specification of the syntax and semantics of bioinformatics resources, and enables cataloguing efforts such as bio.tools that help scientists to find, comprehend, and compare resources. The use of biotoolsSchema in bio.tools promotes the FAIRness of research software, a key element of open and reproducible developments for data-intensive sciences.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/YDMHVWVA/Ison e.a. - 2021 - biotoolsSchema a formalized schema for bioinforma.pdf}
}

@article{isonCommunityCurationBioinformatics2020,
  title = {Community Curation of Bioinformatics Software and Data Resources},
  author = {Ison, Jon and M{\'e}nager, Herv{\'e} and Brancotte, Bryan and Jaaniso, Erik and Salumets, Ahto and Ra{\v c}ek, Tom{\'a}{\v s} and Lamprecht, Anna-Lena and Palmblad, Magnus and Kala{\v s}, Mat{\'u}{\v s} and Chmura, Piotr and Hancock, John M and Schw{\"a}mmle, Veit and Ienasescu, Hans-Ioan},
  year = {2020},
  month = sep,
  journal = {Briefings in Bioinformatics},
  volume = {21},
  number = {5},
  pages = {1697--1705},
  issn = {1477-4054},
  doi = {10.1093/bib/bbz075},
  urldate = {2022-05-04},
  abstract = {The corpus of bioinformatics resources is huge and expanding rapidly, presenting life scientists with a growing challenge in selecting tools that fit the desired purpose. To address this, the European Infrastructure for Biological Information is supporting a systematic approach towards a comprehensive registry of tools and databases for all domains of bioinformatics, provided under a single portal (https://bio.tools). We describe here the practical means by which scientific communities, including individual developers and projects, through major service providers and research infrastructures, can describe their own bioinformatics resources and share these via bio.tools.},
  langid = {english},
  keywords = {biotoolsSchema,software citation,software registry},
  file = {/Users/r.d.wit/Zotero/storage/YDV46RCD/Ison e.a. - 2020 - Community curation of bioinformatics software and .pdf}
}

@article{isonEDAMOntologyBioinformatics2013,
  title = {{{EDAM}}: An Ontology of Bioinformatics Operations, Types of Data and Identifiers, Topics and Formats},
  shorttitle = {{{EDAM}}},
  author = {Ison, J. and Kalas, M. and Jonassen, I. and Bolser, D. and Uludag, M. and McWilliam, H. and Malone, J. and Lopez, R. and Pettifer, S. and Rice, P.},
  year = {2013},
  month = may,
  journal = {Bioinformatics},
  volume = {29},
  number = {10},
  pages = {1325--1332},
  issn = {1367-4803, 1460-2059},
  doi = {10.1093/bioinformatics/btt113},
  urldate = {2022-03-17},
  abstract = {Motivation: Advancing the search, publication and integration of bioinformatics tools and resources demands consistent machine-understandable descriptions. A comprehensive ontology allowing such descriptions is therefore required.},
  langid = {english},
  keywords = {edam,ontologies},
  file = {/Users/r.d.wit/Zotero/storage/WE73MEUF/Ison e.a. - 2013 - EDAM an ontology of bioinformatics operations, ty.pdf}
}

@article{isonToolsDataServices2016,
  title = {Tools and Data Services Registry: A Community Effort to Document Bioinformatics Resources},
  shorttitle = {Tools and Data Services Registry},
  author = {Ison, Jon and Rapacki, Kristoffer and M{\'e}nager, Herv{\'e} and Kala{\v s}, Mat{\'u}{\v s} and Rydza, Emil and Chmura, Piotr and Anthon, Christian and Beard, Niall and Berka, Karel and Bolser, Dan and Booth, Tim and Bretaudeau, Anthony and Brezovsky, Jan and Casadio, Rita and Cesareni, Gianni and Coppens, Frederik and Cornell, Michael and Cuccuru, Gianmauro and Davidsen, Kristian and Vedova, Gianluca Della and Dogan, Tunca and {Doppelt-Azeroual}, Olivia and Emery, Laura and Gasteiger, Elisabeth and Gatter, Thomas and Goldberg, Tatyana and Grosjean, Marie and Gr{\"u}ning, Bj{\"o}rn and {Helmer-Citterich}, Manuela and Ienasescu, Hans and Ioannidis, Vassilios and Jespersen, Martin Closter and Jimenez, Rafael and Juty, Nick and Juvan, Peter and Koch, Maximilian and Laibe, Camille and Li, Jing-Woei and Licata, Luana and Mareuil, Fabien and Mi{\v c}eti{\'c}, Ivan and Friborg, Rune M{\o}llegaard and Moretti, Sebastien and Morris, Chris and M{\"o}ller, Steffen and Nenadic, Aleksandra and Peterson, Hedi and Profiti, Giuseppe and Rice, Peter and Romano, Paolo and Roncaglia, Paola and Saidi, Rabie and Schafferhans, Andrea and Schw{\"a}mmle, Veit and Smith, Callum and Sperotto, Maria Maddalena and Stockinger, Heinz and Va{\v r}ekov{\'a}, Radka Svobodov{\'a} and Tosatto, Silvio C.E. and {de~la~Torre}, Victor and Uva, Paolo and Via, Allegra and Yachdav, Guy and Zambelli, Federico and Vriend, Gert and Rost, Burkhard and Parkinson, Helen and L{\o}ngreen, Peter and Brunak, S{\o}ren},
  year = {2016},
  month = jan,
  journal = {Nucleic Acids Research},
  volume = {44},
  number = {D1},
  pages = {D38-D47},
  issn = {0305-1048, 1362-4962},
  doi = {10.1093/nar/gkv1116},
  urldate = {2022-04-18},
  abstract = {Life sciences are yielding huge data sets that underpin scientific discoveries fundamental to improvement in human health, agriculture and the environment. In support of these discoveries, a plethora of databases and tools are deployed, in technically complex and diverse implementations, across a spectrum of scientific disciplines. The corpus of documentation of these resources is fragmented across the Web, with much redundancy, and has lacked a common standard of information. The outcome is that scientists must often struggle to find, understand, compare and use the best resources for the task at hand.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/DEWFRKEX/Ison e.a. - 2016 - Tools and data services registry a community effo.pdf}
}

@article{ivieReproducibilityScientificComputing2019,
  title = {Reproducibility in {{Scientific Computing}}},
  author = {Ivie, Peter and Thain, Douglas},
  year = {2019},
  month = may,
  journal = {ACM Computing Surveys},
  volume = {51},
  number = {3},
  pages = {1--36},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3186266},
  urldate = {2022-06-02},
  abstract = {Reproducibility is widely considered to be an essential requirement of the scientific process. However, a number of serious concerns have been raised recently, questioning whether today's computational work is adequately reproducible. In principle, it should be possible to specify a computation to sufficient detail that anyone should be able to reproduce it exactly. But in practice, there are fundamental, technical, and social barriers to doing so. The many objectives and meanings of reproducibility are discussed within the context of scientific computing. Technical barriers to reproducibility are described, extant approaches surveyed, and open areas of research are identified.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/36AJSYYF/Ivie en Thain - 2019 - Reproducibility in Scientific Computing.pdf}
}

@article{kabschDictionaryProteinSecondary1983,
  title = {Dictionary of Protein Secondary Structure: {{Pattern}} Recognition of Hydrogen-Bonded and Geometrical Features},
  shorttitle = {Dictionary of Protein Secondary Structure},
  author = {Kabsch, Wolfgang and Sander, Christian},
  year = {1983},
  month = dec,
  journal = {Biopolymers},
  volume = {22},
  number = {12},
  pages = {2577--2637},
  issn = {0006-3525, 1097-0282},
  doi = {10.1002/bip.360221211},
  urldate = {2022-08-08},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/KTJKAQEC/Kabsch en Sander - 1983 - Dictionary of protein secondary structure Pattern.pdf}
}

@article{kanwalInvestigatingReproducibilityTracking2017,
  title = {Investigating Reproducibility and Tracking Provenance \textendash{} {{A}} Genomic Workflow Case Study},
  author = {Kanwal, Sehrish and Khan, Farah Zaib and Lonie, Andrew and Sinnott, Richard O.},
  year = {2017},
  month = dec,
  journal = {BMC Bioinformatics},
  volume = {18},
  number = {1},
  pages = {337},
  issn = {1471-2105},
  doi = {10.1186/s12859-017-1747-0},
  urldate = {2022-06-17},
  abstract = {Background: Computational bioinformatics workflows are extensively used to analyse genomics data, with different approaches available to support implementation and execution of these workflows. Reproducibility is one of the core principles for any scientific workflow and remains a challenge, which is not fully addressed. This is due to incomplete understanding of reproducibility requirements and assumptions of workflow definition approaches. Provenance information should be tracked and used to capture all these requirements supporting reusability of existing workflows. Results: We have implemented a complex but widely deployed bioinformatics workflow using three representative approaches to workflow definition and execution. Through implementation, we identified assumptions implicit in these approaches that ultimately produce insufficient documentation of workflow requirements resulting in failed execution of the workflow. This study proposes a set of recommendations that aims to mitigate these assumptions and guides the scientific community to accomplish reproducible science, hence addressing reproducibility crisis. Conclusions: Reproducing, adapting or even repeating a bioinformatics workflow in any environment requires substantial technical knowledge of the workflow execution environment, resolving analysis assumptions and rigorous compliance with reproducibility requirements. Towards these goals, we propose conclusive recommendations that along with an explicit declaration of workflow specification would result in enhanced reproducibility of computational genomic analyses.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/KJP7KY25/Kanwal e.a. - 2017 - Investigating reproducibility and tracking provena.pdf}
}

@article{katzSoftwareCitationImplementation,
  title = {Software {{Citation Implementation Challenges}}},
  author = {Katz, Daniel S and Bouquin, Daina and Hong, Neil P Chue and Hausman, Jessica and Chivvis, Daniel and Clark, Tim and Crosas, Merc{\`e} and Druskat, Stephan and Fenner, Martin and {Gonzalez-Beltran}, Alejandra and Gruenpeter, Morane and Habermann, Ted and Haines, Robert and Harrison, Melissa and Henneken, Edwin and Hwang, Lorraine and Jones, Matthew B and Kelly, Alastair A and Kennedy, N and Leinweber, Katrin and Rios, Fernando and Robinson, Carly B and Todorov, Ilian and Wu, Mingfang and Zhang, Qian},
  pages = {26},
  langid = {english},
  keywords = {software citation},
  file = {/Users/r.d.wit/Zotero/storage/X4F69KYR/Katz e.a. - Software Citation Implementation Challenges.pdf}
}

@incollection{katzSoftwareCitationTheory2018,
  title = {Software {{Citation}} in {{Theory}} and {{Practice}}},
  booktitle = {Mathematical {{Software}} \textendash{} {{ICMS}} 2018},
  author = {Katz, Daniel S. and Chue Hong, Neil P.},
  editor = {Davenport, James H. and Kauers, Manuel and Labahn, George and Urban, Josef},
  year = {2018},
  volume = {10931},
  pages = {289--296},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-96418-8_34},
  urldate = {2022-04-19},
  abstract = {In most fields, computational models and data analysis have become a significant part of how research is performed, in addition to the more traditional theory and experiment. Mathematics is no exception to this trend. While the system of publication and credit for theory and experiment (journals and books, often monographs) has developed and has become an expected part of the culture, how research is shared and how candidates for hiring, promotion are evaluated, software (and data) do not have the same history. A group working as part of the FORCE11 community developed a set of principles for software citation that fit software into the journal citation system, allow software to be published and then cited, and there are now over 50,000 DOIs that have been issued for software. However, some challenges remain, including: promoting the idea of software citation to developers and users; collaborating with publishers to ensure that systems collect and retain required metadata; ensuring that the rest of the scholarly infrastructure, particularly indexing sites, include software; working with communities so that software efforts count; and understanding how best to cite software that has not been published.},
  isbn = {978-3-319-96417-1 978-3-319-96418-8},
  langid = {english},
  keywords = {FORCE11,software citation},
  file = {/Users/r.d.wit/Zotero/storage/YV9SM94X/Katz en Chue Hong - 2018 - Software Citation in Theory and Practice.pdf}
}

@article{katzTakingFreshLook2021,
  title = {Taking a Fresh Look at {{FAIR}} for Research Software},
  author = {Katz, Daniel S. and Gruenpeter, Morane and Honeyman, Tom},
  year = {2021},
  month = mar,
  journal = {Patterns},
  volume = {2},
  number = {3},
  pages = {100222},
  issn = {26663899},
  doi = {10.1016/j.patter.2021.100222},
  urldate = {2022-04-25},
  langid = {english},
  keywords = {FAIR,software},
  file = {/Users/r.d.wit/Zotero/storage/DBT2RYTL/Katz e.a. - 2021 - Taking a fresh look at FAIR for research software.pdf}
}

@misc{kelloggJSONLDJSONbasedSerialization2020,
  title = {{{JSON-LD}} 1.1: {{A JSON-based Serialization}} for {{Linked Data}}},
  author = {Kellogg, Gregg and Champin, Pierre-Antoine and Longley, Dave and Sporny, Manu and Lanthaler, Markus and Lindstr{\"o}m, Niklas},
  year = {2020},
  urldate = {2022-08-01},
  howpublished = {https://www.w3.org/TR/json-ld11/},
  keywords = {ontologies,ontology,provenance,wfdesc,workflows}
}

@article{kesselHowFixTuberculosis2019,
  title = {How to Fix Tuberculosis {{R}}\&{{D}}\textemdash the Community Speaks},
  author = {Kessel, Mark and Rana, Bhavana and Boehme, Catharina},
  year = {2019},
  month = apr,
  journal = {Nature Biotechnology},
  volume = {37},
  number = {4},
  pages = {350--351},
  issn = {1087-0156, 1546-1696},
  doi = {10.1038/s41587-019-0051-0},
  urldate = {2022-04-27},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/BDHUEU2V/Kessel e.a. - 2019 - How to fix tuberculosis R&D—the community speaks.pdf}
}

@incollection{khanChapter25Antibodies2014,
  title = {Chapter 25 - {{Antibodies}} and {{Their Applications}}},
  booktitle = {Animal {{Biotechnology}}},
  author = {Khan, Fahim Halim},
  year = {2014},
  pages = {473--490},
  publisher = {{Academic Press}},
  address = {{San Diego}},
  urldate = {2022-04-19},
  abstract = {With the application of hybridoma technology, advances in the diagnosis and therapeutics of clinically important diseases have started to appear. Monoclonal antibodies have started to displace antiquated polyclonal antibodies, and new recombinant products such as chimeric and humanized versions of antibody constructs (scFv, dsFv, diabodies, and bispecific antibodies) are likely to become future immunological reagents that are going to revolutionize diagnostics and therapeutics for life-threatening diseases.},
  langid = {english},
  keywords = {FORCE11,software citation},
  annotation = {https://doi.org/10.1016/B978-0-12-416002-6.00025-0}
}

@article{khanSharingInteroperableWorkflow2019,
  title = {Sharing Interoperable Workflow Provenance: {{A}} Review of Best Practices and Their Practical Application in {{CWLProv}}},
  shorttitle = {Sharing Interoperable Workflow Provenance},
  author = {Khan, Farah Zaib and {Soiland-Reyes}, Stian and Sinnott, Richard O and Lonie, Andrew and Goble, Carole and Crusoe, Michael R},
  year = {2019},
  month = nov,
  journal = {GigaScience},
  volume = {8},
  number = {11},
  pages = {giz095},
  issn = {2047-217X},
  doi = {10.1093/gigascience/giz095},
  urldate = {2022-05-03},
  abstract = {Background: The automation of data analysis in the form of scientific workflows has become a widely adopted practice in many fields of research. Computationally driven data-intensive experiments using workflows enable automation, scaling, adaptation, and provenance support. However, there are still several challenges associated with the effective sharing, publication, and reproducibility of such workflows due to the incomplete capture of provenance and lack of interoperability between different technical (software) platforms. Results: Based on best-practice recommendations identified from the literature on workflow design, sharing, and publishing, we define a hierarchical provenance framework to achieve uniformity in provenance and support comprehensive and fully re-executable workflows equipped with domain-specific information. To realize this framework, we present CWLProv, a standard-based format to represent any workflow-based computational analysis to produce workflow output artefacts that satisfy the various levels of provenance. We use open source community-driven standards, interoperable workflow definitions in Common Workflow Language (CWL), structured provenance representation using the W3C PROV model, and resource aggregation and sharing as workflow-centric research objects generated along with the final outputs of a given workflow enactment. We demonstrate the utility of this approach through a practical implementation of CWLProv and evaluation using real-life genomic workflows developed by independent groups. Conclusions: The underlying principles of the standards utilized by CWLProv enable semantically rich and executable research objects that capture computational workflows with retrospective provenance such that any platform supporting CWL will be able to understand the analysis, reuse the methods for partial reruns, or reproduce the analysis to validate the published findings.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/6GVGZ3IL/Khan e.a. - 2019 - Sharing interoperable workflow provenance A revie.pdf}
}

@article{koehorstComparison432Pseudomonas2016,
  title = {Comparison of 432 {{Pseudomonas}} Strains through Integration of Genomic, Functional, Metabolic and Expression Data},
  author = {Koehorst, Jasper J. and {van Dam}, Jesse C. J. and {van Heck}, Ruben G. A. and Saccenti, Edoardo and {dos Santos}, Vitor A. P. Martins and {Suarez-Diez}, Maria and Schaap, Peter J.},
  year = {2016},
  month = dec,
  journal = {Scientific Reports},
  volume = {6},
  number = {1},
  pages = {38699},
  issn = {2045-2322},
  doi = {10.1038/srep38699},
  urldate = {2023-05-10},
  abstract = {Abstract                            Pseudomonas               is a highly versatile genus containing species that can be harmful to humans and plants while others are widely used for bioengineering and bioremediation. We analysed 432 sequenced               Pseudomonas               strains by integrating results from a large scale functional comparison using protein domains with data from six metabolic models, nearly a thousand transcriptome measurements and four large scale transposon mutagenesis experiments. Through heterogeneous data integration we linked gene essentiality, persistence and expression variability. The pan-genome of               Pseudomonas               is closed indicating a limited role of horizontal gene transfer in the evolutionary history of this genus. A large fraction of essential genes are highly persistent, still non essential genes represent a considerable fraction of the core-genome. Our results emphasize the power of integrating large scale comparative functional genomics with heterogeneous data for exploring bacterial diversity and versatility.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/XYBU2W7Y/Koehorst et al. - 2016 - Comparison of 432 Pseudomonas strains through inte.pdf}
}

@article{kotliarCWLAirflowLightweightPipeline2019,
  title = {{{CWL-Airflow}}: A Lightweight Pipeline Manager Supporting {{Common Workflow Language}}},
  shorttitle = {{{CWL-Airflow}}},
  author = {Kotliar, Michael and Kartashov, Andrey V and Barski, Artem},
  year = {2019},
  month = jul,
  journal = {GigaScience},
  volume = {8},
  number = {7},
  pages = {giz084},
  issn = {2047-217X},
  doi = {10.1093/gigascience/giz084},
  urldate = {2022-08-04},
  abstract = {Background: Massive growth in the amount of research data and computational analysis has led to increased use of pipeline managers in biomedical computational research. However, each of the {$>$}100 such managers uses its own way to describe pipelines, leading to difficulty porting workflows to different environments and therefore poor reproducibility of computational studies. For this reason, the Common Workflow Language (CWL) was recently introduced as a specification for platform-independent workflow description, and work began to transition existing pipelines and workflow managers to CWL. Findings: Herein, we present CWL-Airflow, a package that adds support for CWL to the Apache Airflow pipeline manager. CWL-Airflow uses CWL version 1.0 specification and can run workflows on stand-alone MacOS/Linux servers, on clusters, or on a variety of cloud platforms. A sample CWL pipeline for processing of chromatin immunoprecipitation sequencing data is provided. Conclusions: CWL-Airflow will provide users with the features of a fully fledged pipeline manager and the ability to execute CWL workflows anywhere Airflow can run\textemdash from a laptop to a cluster or cloud environment. CWL-Airflow is available under Apache License, version 2.0 (Apache-2.0), and can be downloaded from https://barski-lab.github.io/cwl-airflow, https://scicrunch.org/resolver/RRID:SCR 017196.},
  langid = {english},
  keywords = {common workflow language,workflow management system},
  file = {/Users/r.d.wit/Zotero/storage/K2K5WXUH/Kotliar e.a. - 2019 - CWL-Airflow a lightweight pipeline manager suppor.pdf}
}

@article{krafczykLearningReproducingComputational2021,
  title = {Learning from Reproducing Computational Results: Introducing Three Principles and the {{{\emph{Reproduction Package}}}}},
  shorttitle = {Learning from Reproducing Computational Results},
  author = {Krafczyk, M. S. and Shi, A. and Bhaskar, A. and Marinov, D. and Stodden, V.},
  year = {2021},
  month = may,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {379},
  number = {2197},
  pages = {rsta.2020.0069, 20200069},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2020.0069},
  urldate = {2022-06-02},
  abstract = {We carry out efforts to reproduce computational results for seven published articles and identify barriers to computational reproducibility. We then derive three principles to guide the practice and dissemination of reproducible computational research: (i) Provide transparency regarding how computational results are produced; (ii) When writing and releasing research software, aim for ease of (re-)executability; (iii) Make any code upon which the results rely as deterministic as possible. We then exemplify these three principles with 12 specific guidelines for their implementation in practice. We illustrate the three principles of reproducible research with a series of vignettes from our experimental reproducibility work. We define a novel               Reproduction Package               , a formalism that specifies a structured way to share computational research artifacts that implements the guidelines generated from our reproduction efforts to allow others to build, reproduce and extend computational science. We make our reproduction efforts in this paper publicly available as exemplar               Reproduction Packages               .                                         This article is part of the theme issue `Reliability and reproducibility in computational science: implementing verification, validation and uncertainty quantification               in silico               '.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/U7URHLFA/Krafczyk e.a. - 2021 - Learning from reproducing computational results i.pdf}
}

@misc{kuhnNanopublicationsGrowingResource2018,
  title = {Nanopublications: {{A Growing Resource}} of {{Provenance-Centric Scientific Linked Data}}},
  shorttitle = {Nanopublications},
  author = {Kuhn, Tobias and {Mero{\~n}o-Pe{\~n}uela}, Albert and Malic, Alexander and Poelen, Jorrit H. and Hurlbert, Allen H. and Ortiz, Emilio Centeno and Furlong, Laura I. and {Queralt-Rosinach}, N{\'u}ria and Chichester, Christine and Banda, Juan M. and Willighagen, Egon and Ehrhart, Friederike and Evelo, Chris and Malas, Tareq B. and Dumontier, Michel},
  year = {2018},
  month = sep,
  number = {arXiv:1809.06532},
  eprint = {1809.06532},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-12},
  abstract = {Nanopublications are a Linked Data format for scholarly data publishing that has received considerable uptake in the last few years. In contrast to the common Linked Data publishing practice, nanopublications work at the granular level of atomic information snippets and provide a consistent container format to attach provenance and metadata at this atomic level. While the nanopublications format is domain-independent, the datasets that have become available in this format are mostly from Life Science domains, including data about diseases, genes, proteins, drugs, biological pathways, and biotic interactions. More than 10 million such nanopublications have been published, which now form a valuable resource for studies on the domain level of the given Life Science domains as well as on the more technical levels of provenance modeling and heterogeneous Linked Data. We provide here an overview of this combined nanopublication dataset, show the results of some overarching analyses, and describe how it can be accessed and queried.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Digital Libraries,Human Protein Atlas (HPA),nanopublications,User Centric Data Science Group},
  file = {/Users/r.d.wit/Zotero/storage/8CADFUG8/Kuhn et al. - 2018 - Nanopublications A Growing Resource of Provenance.pdf}
}

@article{kurtzerSingularityScientificContainers2017,
  title = {Singularity: {{Scientific}} Containers for Mobility of Compute},
  shorttitle = {Singularity},
  author = {Kurtzer, Gregory M. and Sochat, Vanessa and Bauer, Michael W.},
  editor = {Gursoy, Attila},
  year = {2017},
  month = may,
  journal = {PLOS ONE},
  volume = {12},
  number = {5},
  pages = {e0177459},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0177459},
  urldate = {2022-05-06},
  abstract = {Here we present Singularity, software developed to bring containers and reproducibility to scientific computing. Using Singularity containers, developers can work in reproducible environments of their choosing and design, and these complete environments can easily be copied and executed on other platforms. Singularity is an open source initiative that harnesses the expertise of system and software engineers and researchers alike, and integrates seamlessly into common workflows for both of these groups. As its primary use case, Singularity brings mobility of computing to both users and HPC centers, providing a secure means to capture and distribute software and compute environments. This ability to create and deploy reproducible environments across these centers, a previously unmet need, makes Singularity a game changing development for computational science.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/7YJGKTRY/Kurtzer e.a. - 2017 - Singularity Scientific containers for mobility of.pdf}
}

@article{lamprechtFAIRPrinciplesResearch2020,
  title = {Towards {{FAIR}} Principles for Research Software},
  author = {Lamprecht, Anna-Lena and Garcia, Leyla and Kuzak, Mateusz and Martinez, Carlos and Arcila, Ricardo and Martin Del Pico, Eva and Dominguez Del Angel, Victoria and {van de Sandt}, Stephanie and Ison, Jon and Martinez, Paula Andrea and McQuilton, Peter and Valencia, Alfonso and Harrow, Jennifer and Psomopoulos, Fotis and Gelpi, Josep Ll. and Chue Hong, Neil and Goble, Carole and {Capella-Gutierrez}, Salvador},
  editor = {Groth, Paul and Groth, Paul and Dumontier, Michel},
  year = {2020},
  month = jun,
  journal = {Data Science},
  volume = {3},
  number = {1},
  pages = {37--59},
  issn = {24518492, 24518484},
  doi = {10.3233/DS-190026},
  urldate = {2022-04-25},
  abstract = {The FAIR Guiding Principles, published in 2016, aim to improve the findability, accessibility, interoperability and reusability of digital research objects for both humans and machines. Until now the FAIR principles have been mostly applied to research data. The ideas behind these principles are, however, also directly relevant to research software. Hence there is a distinct need to explore how the FAIR principles can be applied to software. In this work, we aim to summarize the current status of the debate around FAIR and software, as basis for the development of community-agreed principles for FAIR research software in the future. We discuss what makes software different from data with regard to the application of the FAIR principles, and which desired characteristics of research software go beyond FAIR. Then we present an analysis of where the existing principles can directly be applied to software, where they need to be adapted or reinterpreted, and where the definition of additional principles is required. Here interoperability has proven to be the most challenging principle, calling for particular attention in future discussions. Finally, we outline next steps on the way towards definite FAIR principles for research software.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/V2JQX6KX/Lamprecht e.a. - 2020 - Towards FAIR principles for research software.pdf}
}

@article{leipzigRoleMetadataReproducible2021,
  title = {The Role of Metadata in Reproducible Computational Research},
  author = {Leipzig, Jeremy and N{\"u}st, Daniel and Hoyt, Charles Tapley and Ram, Karthik and Greenberg, Jane},
  year = {2021},
  month = sep,
  journal = {Patterns},
  volume = {2},
  number = {9},
  pages = {100322},
  issn = {26663899},
  doi = {10.1016/j.patter.2021.100322},
  urldate = {2022-08-22},
  abstract = {Reproducible computational research (RCR) is the keystone of the scientific method for in silico analyses, packaging the transformation of raw data to published results. In addition to its role in research integrity, improving the reproducibility of scientific studies can accelerate evaluation and reuse. This potential and wide support for the FAIR principles have motivated interest in metadata standards supporting reproducibility. Metadata provide context and provenance to raw data and methods and are essential to both discovery and validation. Despite this shared connection with scientific data, few studies have explicitly described how metadata enable reproducible computational research. This review employs a functional content analysis to identify metadata standards that support reproducibility across an analytic stack consisting of input data, tools, notebooks, pipelines, and publications. Our review provides background context, explores gaps, and discovers component trends of embeddedness and methodology weight from which we derive recommendations for future work.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/H58L9BV4/Leipzig e.a. - 2021 - The role of metadata in reproducible computational.pdf}
}

@incollection{ludascherBriefTourProvenance2016,
  title = {A {{Brief Tour Through Provenance}} in {{Scientific Workflows}} and {{Databases}}},
  booktitle = {Building {{Trust}} in {{Information}}},
  author = {Lud{\"a}scher, Bertram},
  editor = {Lemieux, Victoria L.},
  year = {2016},
  pages = {103--126},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-40226-0_7},
  urldate = {2022-03-03},
  abstract = {Within computer science, the term provenance has multiple meanings, due to different motivations, perspectives, and assumptions prevalent in the respective communities. This chapter provides a high-level ``sightseeing tour'' of some of those different notions and uses of provenance in scientific workflows and databases.},
  isbn = {978-3-319-40225-3 978-3-319-40226-0},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/DRE5CPUF/Ludäscher - 2016 - A Brief Tour Through Provenance in Scientific Work.pdf}
}

@article{martoranaAligningRestrictedAccess2022,
  title = {Aligning Restricted Access Data with {{FAIR}}: A Systematic Review},
  shorttitle = {Aligning Restricted Access Data with {{FAIR}}},
  author = {Martorana, Margherita and Kuhn, Tobias and Siebes, Ronald and {van Ossenbruggen}, Jacco},
  year = {2022},
  month = jul,
  journal = {PeerJ Computer Science},
  volume = {8},
  pages = {e1038},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.1038},
  urldate = {2022-10-11},
  abstract = {Understanding the complexity of restricted research data is vitally important in the current new era of Open Science. While the FAIR Guiding Principles have been introduced to help researchers to make data Findable, Accessible, Interoperable and Reusable, it is still unclear how the notions of FAIR and Openness can be applied in the context of restricted data. Many methods have been proposed in support of the implementation of the principles, but there is yet no consensus among the scientific community as to the suitable mechanisms of making restricted data FAIR. We present here a systematic literature review to identify the methods applied by scientists when researching restricted data in a FAIR-compliant manner in the context of the FAIR principles. Through the employment of a descriptive and iterative study design, we aim to answer the following three questions: (1) What methods have been proposed to apply the FAIR principles to restricted data?, (2) How can the relevant aspects of the methods proposed be categorized?, (3) What is the maturity of the methods proposed in applying the FAIR principles to restricted data?. After analysis of the 40 included publications, we noticed that the methods found, reflect the stages of the Data Life Cycle, and can be divided into the following Classes: Data Collection, Metadata Representation, Data Processing, Anonymization, Data Publication, Data Usage and Post Data Usage. We observed that a large number of publications used `Access Control` and `Usage and License Terms' methods, while others such as `Embargo on Data Release' and the use of `Synthetic Data' were used in fewer instances. In conclusion, we are presenting the first extensive literature review on the methods applied to confidential data in the context of FAIR, providing a comprehensive conceptual framework for future research on restricted access data.},
  langid = {english},
  keywords = {TBR},
  file = {/Users/r.d.wit/Zotero/storage/6VX2TYFT/Martorana e.a. - 2022 - Aligning restricted access data with FAIR a syste.pdf}
}

@article{michelBioschemasSchemaOrg2018,
  title = {Bioschemas \& {{Schema}}.Org: A {{Lightweight Semantic Layer}} for {{Life Sciences Websites}}},
  shorttitle = {Bioschemas \& {{Schema}}.Org},
  author = {Michel, Franck and {The Bioschemas Community}},
  year = {2018},
  month = may,
  journal = {Biodiversity Information Science and Standards},
  volume = {2},
  pages = {e25836},
  issn = {2535-0897},
  doi = {10.3897/biss.2.25836},
  urldate = {2022-05-04},
  abstract = {Web portals are commonly used to expose and share scientific data. They enable end users to find, organize and obtain data relevant to their interests. With the continuous growth of data across all science domains, researchers commonly find themselves overwhelmed as finding, retrieving and making sense of data becomes increasingly difficult. Search engines can help find relevant websites, but the short summarizations they provide in results lists are often little informative on how relevant a website is with respect to research interests.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/LUCAQKVF/Michel en The Bioschemas Community - 2018 - Bioschemas &amp\; Schema.org a Lightweight Semanti.pdf}
}

@incollection{missierLifecycleProvenanceMetadata2016,
  title = {The {{Lifecycle}} of {{Provenance Metadata}} and {{Its Associated Challenges}} and {{Opportunities}}},
  booktitle = {Building {{Trust}} in {{Information}}},
  author = {Missier, Paolo},
  editor = {Lemieux, Victoria L.},
  year = {2016},
  pages = {127--137},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-40226-0_8},
  urldate = {2022-03-03},
  abstract = {This chapter outlines some of the challenges and opportunities associated with adopting provenance principles (Cheney et al., Dagstuhl Reports 2(2):84\textendash 113, 2012) and standards (Moreau et al., Web Semant. Sci. Serv. Agents World Wide Web, 2015) in a variety of disciplines, including data publication and reuse, and information sciences.},
  isbn = {978-3-319-40225-3 978-3-319-40226-0},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/FSDDN8F2/Missier - 2016 - The Lifecycle of Provenance Metadata and Its Assoc.pdf}
}

@article{mitchellEBIMetagenomics20172018,
  title = {{{EBI Metagenomics}} in 2017: Enriching the Analysis of Microbial Communities, from Sequence Reads to Assemblies},
  shorttitle = {{{EBI Metagenomics}} in 2017},
  author = {Mitchell, Alex L and Scheremetjew, Maxim and Denise, Hubert and Potter, Simon and Tarkowska, Aleksandra and Qureshi, Matloob and Salazar, Gustavo A and Pesseat, Sebastien and Boland, Miguel A and Hunter, Fiona~M~I and {ten~Hoopen}, Petra and Alako, Blaise and Amid, Clara and Wilkinson, Darren J and Curtis, Thomas P and Cochrane, Guy and Finn, Robert D},
  year = {2018},
  month = jan,
  journal = {Nucleic Acids Research},
  volume = {46},
  number = {D1},
  pages = {D726-D735},
  issn = {0305-1048, 1362-4962},
  doi = {10.1093/nar/gkx967},
  urldate = {2022-03-10},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/QVPLK9R3/Mitchell e.a. - 2018 - EBI Metagenomics in 2017 enriching the analysis o.pdf}
}

@article{mitchellMGnifyMicrobiomeAnalysis2019,
  title = {{{MGnify}}: The Microbiome Analysis Resource in 2020},
  shorttitle = {{{MGnify}}},
  author = {Mitchell, Alex L and Almeida, Alexandre and Beracochea, Martin and Boland, Miguel and Burgin, Josephine and Cochrane, Guy and Crusoe, Michael R and Kale, Varsha and Potter, Simon C and Richardson, Lorna J and Sakharova, Ekaterina and Scheremetjew, Maxim and Korobeynikov, Anton and Shlemov, Alex and Kunyavskaya, Olga and Lapidus, Alla and Finn, Robert D},
  year = {2019},
  month = nov,
  journal = {Nucleic Acids Research},
  pages = {gkz1035},
  issn = {0305-1048, 1362-4962},
  doi = {10.1093/nar/gkz1035},
  urldate = {2022-02-21},
  abstract = {MGnify (http://www.ebi.ac.uk/metagenomics) provides a free to use platform for the assembly, analysis and archiving of microbiome data derived from sequencing microbial populations that are present in particular environments. Over the past 2 years, MGnify (formerly EBI Metagenomics) has more than doubled the number of publicly available analysed datasets held within the resource. Recently, an updated approach to data analysis has been unveiled (version 5.0), replacing the previous single pipeline with multiple analysis pipelines that are tailored according to the input data, and that are formally described using the Common Workflow Language, enabling greater provenance, reusability, and reproducibility. MGnify's new analysis pipelines offer additional approaches for taxonomic assertions based on ribosomal internal transcribed spacer regions (ITS1/2) and expanded protein functional annotations. Biochemical pathways and systems predictions have also been added for assembled contigs. MGnify's growing focus on the assembly of metagenomic data has also seen the number of datasets it has assembled and analysed increase six-fold. The non-redundant protein database constructed from the proteins encoded by these assemblies now exceeds 1 billion sequences. Meanwhile, a newly developed contig viewer provides fine-grained visualisation of the assembled contigs and their enriched annotations.},
  langid = {english},
  keywords = {MGnify},
  file = {/Users/r.d.wit/Zotero/storage/P6F7AZTZ/Mitchell e.a. - 2019 - MGnify the microbiome analysis resource in 2020.pdf}
}


@TechReport{moreauPROVDMPROVData2013,
  author      = "Paolo Missier and Luc Moreau",
  title       = "{PROV}-DM: The {PROV} Data Model",
  month       = apr,
  url        = "https://www.w3.org/TR/2013/REC-prov-dm-20130430/",
  year        = "2013",
  bibsource   = "https://w2.syronex.com/jmr/w3c-biblio",
  type        = "{W3C} Recommendation",
  institution = "W3C",
}

@article{nijsseFAIRDataStation2022,
  title = {{{FAIR}} Data Station for Lightweight Metadata Management and Validation of Omics Studies},
  author = {Nijsse, Bart and Schaap, Peter J and Koehorst, Jasper J},
  year = {2022},
  month = dec,
  journal = {GigaScience},
  volume = {12},
  pages = {giad014},
  issn = {2047-217X},
  doi = {10.1093/gigascience/giad014},
  urldate = {2023-03-17},
  abstract = {Background: The life sciences are one of the biggest suppliers of scientific data. Reusing and connecting these data can uncover hidden insights and lead to new concepts. Efficient reuse of these datasets is strongly promoted when they are interlinked with a sufficient amount of machine-actionable metadata. While the FAIR (Findable, Accessible, Interoperable, Reusable) guiding principles have been accepted by all stakeholders, in practice, there are only a limited number of easy-to-adopt implementations available that fulfill the needs of data producers.},
  langid = {english},
  keywords = {BioSB Knowledge Graphs 2023 (WUR),Jasper Koehorst,metadata},
  file = {/Users/r.d.wit/Zotero/storage/9SKAMSMP/Nijsse et al. - 2022 - FAIR data station for lightweight metadata managem.pdf}
}

@article{parkInformalDataCitation2018,
  title = {Informal Data Citation for Data Sharing and Reuse Is More Common than Formal Data Citation in Biomedical Fields},
  author = {Park, Hyoungjoo and You, Sukjin and Wolfram, Dietmar},
  year = {2018},
  month = nov,
  journal = {Journal of the Association for Information Science and Technology},
  volume = {69},
  number = {11},
  pages = {1346--1354},
  issn = {23301635},
  doi = {10.1002/asi.24049},
  urldate = {2022-04-21},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/HIPS5ZTU/Park e.a. - 2018 - Informal data citation for data sharing and reuse .pdf}
}

@article{pengReproducibleResearchRetrospective2021,
  title = {Reproducible {{Research}}: {{A Retrospective}}},
  shorttitle = {Reproducible {{Research}}},
  author = {Peng, Roger D. and Hicks, Stephanie C.},
  year = {2021},
  month = apr,
  journal = {Annual Review of Public Health},
  volume = {42},
  number = {1},
  pages = {79--93},
  issn = {0163-7525, 1545-2093},
  doi = {10.1146/annurev-publhealth-012420-105110},
  urldate = {2022-06-02},
  abstract = {Advances in computing technology have spurred two extraordinary phenomena in science: large-scale and high-throughput data collection coupled with the creation and implementation of complex statistical algorithms for data analysis. These two phenomena have brought about tremendous advances in scientific discovery but have raised two serious concerns. The complexity of modern data analyses raises questions about the reproducibility of the analyses, meaning the ability of independent analysts to recreate the results claimed by the original authors using the original data and analysis techniques. Reproducibility is typically thwarted by a lack of availability of the original data and computer code. A more general concern is the replicability of scientific findings, which concerns the frequency with which scientific claims are confirmed by completely independent investigations. Although reproducibility and replicability are related, they focus on different aspects of scientific progress. In this review, we discuss the origins of reproducible research, characterize the current status of reproducibility in public health research, and connect reproducibility to current concerns about the replicability of scientific findings. Finally, we describe a path forward for improving both the reproducibility and replicability of public health research in the future.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/PWC6KE2C/Peng en Hicks - 2021 - Reproducible Research A Retrospective.pdf}
}

@article{perez-riverolTenSimpleRules2016,
  title = {Ten {{Simple Rules}} for {{Taking Advantage}} of {{Git}} and {{GitHub}}},
  author = {{Perez-Riverol}, Yasset and Gatto, Laurent and Wang, Rui and Sachsenberg, Timo and Uszkoreit, Julian and Leprevost, Felipe da Veiga and Fufezan, Christian and Ternent, Tobias and Eglen, Stephen J. and Katz, Daniel S. and Pollard, Tom J. and Konovalov, Alexander and Flight, Robert M. and Blin, Kai and Vizca{\'i}no, Juan Antonio},
  editor = {Markel, Scott},
  year = {2016},
  month = jul,
  journal = {PLOS Computational Biology},
  volume = {12},
  number = {7},
  pages = {e1004947},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004947},
  urldate = {2022-05-06},
  langid = {english},
  keywords = {10 simple rules,git,GitHub},
  file = {/Users/r.d.wit/Zotero/storage/3286N6QH/Perez-Riverol e.a. - 2016 - Ten Simple Rules for Taking Advantage of Git and G.pdf}
}

@article{perezSystematicReviewProvenance2018,
  title = {A Systematic Review of Provenance Systems},
  author = {P{\'e}rez, Beatriz and Rubio, Julio and {S{\'a}enz-Ad{\'a}n}, Carlos},
  year = {2018},
  month = dec,
  journal = {Knowledge and Information Systems},
  volume = {57},
  number = {3},
  pages = {495--543},
  issn = {0219-1377, 0219-3116},
  doi = {10.1007/s10115-018-1164-3},
  urldate = {2022-05-06},
  abstract = {Provenance refers to the entire amount of information, comprising all the elements and their relationships, that contribute to the existence of a piece of data. The knowledge of provenance data allows a great number of benefits such as verifying a product, result reproductivity, sharing and reuse of knowledge, or assessing data quality and validity. With such tangible benefits, it is no wonder that in recent years, research on provenance has grown exponentially, and has been applied to a wide range of different scientific disciplines.},
  langid = {english},
  keywords = {scientific method,systematic review},
  file = {/Users/r.d.wit/Zotero/storage/JYI8HWVC/1608.06897.pdf;/Users/r.d.wit/Zotero/storage/VYU6QAKQ/Pérez e.a. - 2018 - A systematic review of provenance systems.pdf}
}

@article{piccoloToolsTechniquesComputational2016,
  title = {Tools and Techniques for Computational Reproducibility},
  author = {Piccolo, Stephen R. and Frampton, Michael B.},
  year = {2016},
  month = dec,
  journal = {GigaScience},
  volume = {5},
  number = {1},
  pages = {30},
  issn = {2047-217X},
  doi = {10.1186/s13742-016-0135-4},
  urldate = {2022-10-03},
  abstract = {When reporting research findings, scientists document the steps they followed so that others can verify and build upon the research. When those steps have been described in sufficient detail that others can retrace the steps and obtain similar results, the research is said to be reproducible. Computers play a vital role in many research disciplines and present both opportunities and challenges for reproducibility. Computers can be programmed to execute analysis tasks, and those programs can be repeated and shared with others. The deterministic nature of most computer programs means that the same analysis tasks, applied to the same data, will often produce the same outputs. However, in practice, computational findings often cannot be reproduced because of complexities in how software is packaged, installed, and executed\textemdash and because of limitations associated with how scientists document analysis steps. Many tools and techniques are available to help overcome these challenges; here we describe seven such strategies. With a broad scientific audience in mind, we describe the strengths and limitations of each approach, as well as the circumstances under which each might be applied. No single strategy is sufficient for every scenario; thus we emphasize that it is often useful to combine approaches.},
  langid = {english},
  keywords = {TBR},
  file = {/Users/r.d.wit/Zotero/storage/7K4NIDS5/Piccolo en Frampton - 2016 - Tools and techniques for computational reproducibi.pdf}
}

@article{queralt-rosinachApplyingFAIRPrinciples2022,
  title = {Applying the {{FAIR}} Principles to Data in a Hospital: Challenges and Opportunities in a Pandemic},
  shorttitle = {Applying the {{FAIR}} Principles to Data in a Hospital},
  author = {{Queralt-Rosinach}, N{\'u}ria and Kaliyaperumal, Rajaram and Bernab{\'e}, C{\'e}sar H. and Long, Qinqin and Joosten, Simone A. and Van Der Wijk, Henk Jan and Flikkenschild, Erik L.A. and Burger, Kees and Jacobsen, Annika and Mons, Barend and Roos, Marco and {BEAT-COVID Group} and {COVID-19 LUMC Group}},
  year = {2022},
  month = dec,
  journal = {Journal of Biomedical Semantics},
  volume = {13},
  number = {1},
  pages = {12},
  issn = {2041-1480},
  doi = {10.1186/s13326-022-00263-7},
  urldate = {2023-05-12},
  abstract = {Background: The COVID-19 pandemic has challenged healthcare systems and research worldwide. Data is collected all over the world and needs to be integrated and made available to other researchers quickly. However, the various heterogeneous information systems that are used in hospitals can result in fragmentation of health data over multiple data `silos' that are not interoperable for analysis. Consequently, clinical observations in hospitalised patients are not prepared to be reused efficiently and timely. There is a need to adapt the research data management in hospitals to make COVID-19 observational patient data machine actionable, i.e. more Findable, Accessible, Interoperable and Reusable (FAIR) for humans and machines. We therefore applied the FAIR principles in the hospital to make patient data more FAIR. Results: In this paper, we present our FAIR approach to transform COVID-19 observational patient data collected in the hospital into machine actionable digital objects to answer medical doctors' research questions. With this objective, we conducted a coordinated FAIRification among stakeholders based on ontological models for data and metadata, and a FAIR based architecture that complements the existing data management. We applied FAIR Data Points for metadata exposure, turning investigational parameters into a FAIR dataset. We demonstrated that this dataset is machine actionable by means of three different computational activities: federated query of patient data along open existing knowledge sources across the world through the Semantic Web, implementing Web APIs for data query interoperability, and building applications on top of these FAIR patient data for FAIR data analytics in the hospital. Conclusions: Our work demonstrates that a FAIR research data management plan based on ontological models for data and metadata, open Science, Semantic Web technologies, and FAIR Data Points is providing data infrastructure in the hospital for machine actionable FAIR Digital Objects. This FAIR data is prepared to be reused for federated analysis, linkable to other FAIR data such as Linked Open Data, and reusable to develop software applications on top of them for hypothesis generation and knowledge discovery.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/YZDQPVQZ/Queralt-Rosinach et al. - 2022 - Applying the FAIR principles to data in a hospital.pdf}
}

@article{queralt-rosinachStructuredReviewsData2020,
  title = {Structured Reviews for Data and Knowledge-Driven Research},
  author = {{Queralt-Rosinach}, N{\'u}ria and Stupp, Gregory S and Li, Tong Shu and Mayers, Michael and Hoatlin, Maureen E and Might, Matthew and Good, Benjamin M and Su, Andrew I},
  year = {2020},
  volume = {2020},
  abstract = {Hypothesis generation is a critical step in research and a cornerstone in the rare disease field. Research is most efficient when those hypotheses are based on the entirety of knowledge known to date. Systematic review articles are commonly used in biomedicine to summarize existing knowledge and contextualize experimental data. But the information contained within review articles is typically only expressed as free-text, which is difficult to use computationally. Researchers struggle to navigate, collect and remix prior knowledge as it is scattered in several silos without seamless integration and access. This lack of a structured information framework hinders research by both experimental and computational scientists. To better organize knowledge and data, we built a structured review article that is specifically focused on NGLY1 Deficiency, an ultrarare genetic disease first reported in 2012. We represented this structured review as a knowledge graph and then stored this knowledge graph in a Neo4j database to simplify dissemination, querying and visualization of the network. Relative to free-text, this structured review better promotes the principles of findability, accessibility, interoperability and reusability (FAIR). In collaboration with domain experts in NGLY1 Deficiency, we demonstrate how this resource can improve the efficiency and comprehensiveness of hypothesis generation. We also developed a read\textendash write interface that allows domain experts to contribute FAIR structured knowledge to this community resource. In contrast to traditional free-text review articles, this structured review exists as a living knowledge graph that is curated by humans and accessible to computational analyses. Finally, we have generalized this workflow into modular and repurposable components that can be applied to other domain areas. This NGLY1 Deficiency-focused network is publicly available at http://ngly1graph.org/.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/VHAE9CIK/Queralt-Rosinach et al. - 2020 - Structured reviews for data and knowledge-driven r.pdf}
}

@article{ramirezDeepTools2NextGeneration2016,
  title = {{{deepTools2}}: A next Generation Web Server for Deep-Sequencing Data Analysis},
  shorttitle = {{{deepTools2}}},
  author = {Ram{\'i}rez, Fidel and Ryan, Devon P and Gr{\"u}ning, Bj{\"o}rn and Bhardwaj, Vivek and Kilpert, Fabian and Richter, Andreas S and Heyne, Steffen and D{\"u}ndar, Friederike and Manke, Thomas},
  year = {2016},
  month = jul,
  journal = {Nucleic Acids Research},
  volume = {44},
  number = {W1},
  pages = {W160-W165},
  issn = {0305-1048, 1362-4962},
  doi = {10.1093/nar/gkw257},
  urldate = {2022-05-02},
  abstract = {Software Containers are changing the way scientists and researchers develop, deploy and exchange scientific software. They allow labs of all sizes to easily install bioinformatics software, maintain multiple versions of the same software and combine tools into powerful analysis pipelines. However, containers and software packages should be produced under certain rules and standards in order to be reusable, compatible and easy to integrate into pipelines and analysis workflows. Here, we presented a set of recommendations developed by the BioContainers Community to produce standardized bioinformatics packages and containers. These recommendations provide practical guidelines to make bioinformatics software more discoverable, reusable and transparent. They are aimed to guide developers, organisations, journals and funders to increase the quality and sustainability of research software.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/95W8H57D/Ramírez e.a. - 2016 - deepTools2 a next generation web server for deep-.pdf}
}

@article{ramirezDeepTools2NextGeneration2016a,
  title = {{{deepTools2}}: A next Generation Web Server for Deep-Sequencing Data Analysis},
  shorttitle = {{{deepTools2}}},
  author = {Ram{\'i}rez, Fidel and Ryan, Devon P and Gr{\"u}ning, Bj{\"o}rn and Bhardwaj, Vivek and Kilpert, Fabian and Richter, Andreas S and Heyne, Steffen and D{\"u}ndar, Friederike and Manke, Thomas},
  year = {2016},
  month = jul,
  journal = {Nucleic Acids Research},
  volume = {44},
  number = {W1},
  pages = {W160-W165},
  issn = {0305-1048, 1362-4962},
  doi = {10.1093/nar/gkw257},
  urldate = {2022-05-06},
  abstract = {Software Containers are changing the way scientists and researchers develop, deploy and exchange scientific software. They allow labs of all sizes to easily install bioinformatics software, maintain multiple versions of the same software and combine tools into powerful analysis pipelines. However, containers and software packages should be produced under certain rules and standards in order to be reusable, compatible and easy to integrate into pipelines and analysis workflows. Here, we presented a set of recommendations developed by the BioContainers Community to produce standardized bioinformatics packages and containers. These recommendations provide practical guidelines to make bioinformatics software more discoverable, reusable and transparent. They are aimed to guide developers, organisations, journals and funders to increase the quality and sustainability of research software.},
  langid = {english},
  keywords = {check citation},
  file = {/Users/r.d.wit/Zotero/storage/F5CZESIS/Ramírez e.a. - 2016 - deepTools2 a next generation web server for deep-.pdf}
}

@article{registriesNineBestPractices2020,
  title = {Nine {{Best Practices}} for {{Research Software Registries}} and {{Repositories}}: {{A Concise Guide}}},
  shorttitle = {Nine {{Best Practices}} for {{Research Software Registries}} and {{Repositories}}},
  author = {Registries, Task Force on Best Practices for Software and Monteil, Alain and {Gonzalez-Beltran}, Alejandra and Ioannidis, Alexandros and Allen, Alice and Lee, Allen and Bandrowski, Anita and Wilson, Bruce E. and Mecum, Bryce and Du, Cai Fan and Robinson, Carly and Garijo, Daniel and Katz, Daniel S. and Long, David and Milliken, Genevieve and M{\'e}nager, Herv{\'e} and Hausman, Jessica and Spaaks, Jurriaan H. and Fenlon, Katrina and Vanderbilt, Kristin and Hwang, Lorraine and Davis, Lynn and Fenner, Martin and Crusoe, Michael R. and Hucka, Michael and Wu, Mingfang and Hong, Neil Chue and Teuben, Peter and Stall, Shelley and Druskat, Stephan and Carnevale, Ted and Morrell, Thomas},
  year = {2020},
  month = dec,
  journal = {arXiv:2012.13117 [cs]},
  eprint = {2012.13117},
  primaryclass = {cs},
  urldate = {2022-04-21},
  abstract = {Scientific software registries and repositories serve various roles in their respective disciplines. These resources improve software discoverability and research transparency, provide information for software citations, and foster preservation of computational methods that might otherwise be lost over time, thereby supporting research reproducibility and replicability. However, developing these resources takes effort, and few guidelines are available to help prospective creators of registries and repositories. To address this need, we present a set of nine best practices that can help managers define the scope, practices, and rules that govern individual registries and repositories. These best practices were distilled from the experiences of the creators of existing resources, convened by a Task Force of the FORCE11 Software Citation Implementation Working Group during the years 2019-2020. We believe that putting in place specific policies such as those presented here will help scientific software registries and repositories better serve their users and their disciplines.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {check citation,software citation},
  file = {/Users/r.d.wit/Zotero/storage/HAFFJPC6/Registries e.a. - 2020 - Nine Best Practices for Research Software Registri.pdf}
}

@article{remmertHHblitsLightningfastIterative2012,
  title = {{{HHblits}}: Lightning-Fast Iterative Protein Sequence Searching by {{HMM-HMM}} Alignment},
  shorttitle = {{{HHblits}}},
  author = {Remmert, Michael and Biegert, Andreas and Hauser, Andreas and S{\"o}ding, Johannes},
  year = {2012},
  month = feb,
  journal = {Nature Methods},
  volume = {9},
  number = {2},
  pages = {173--175},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/nmeth.1818},
  urldate = {2022-03-07},
  langid = {english},
  keywords = {NIAA workflow},
  file = {/Users/r.d.wit/Zotero/storage/RSGTVPFI/Remmert e.a. - 2012 - HHblits lightning-fast iterative protein sequence.pdf}
}

@article{samuelUnderstandingExperimentsResearch2021,
  title = {Understanding Experiments and Research Practices for Reproducibility: An Exploratory Study},
  shorttitle = {Understanding Experiments and Research Practices for Reproducibility},
  author = {Samuel, Sheeba and {K{\"o}nig-Ries}, Birgitta},
  year = {2021},
  month = apr,
  journal = {PeerJ},
  volume = {9},
  pages = {e11140},
  issn = {2167-8359},
  doi = {10.7717/peerj.11140},
  urldate = {2023-02-12},
  abstract = {Scientific experiments and research practices vary across disciplines. The research practices followed by scientists in each domain play an essential role in the understandability and reproducibility of results. The ``Reproducibility Crisis'', where researchers find difficulty in reproducing published results, is currently faced by several disciplines. To understand the underlying problem in the context of the reproducibility crisis, it is important to first know the different research practices followed in their domain and the factors that hinder reproducibility. We performed an exploratory study by conducting a survey addressed to researchers representing a range of disciplines to understand scientific experiments and research practices for reproducibility. The survey findings identify a reproducibility crisis and a strong need for sharing data, code, methods, steps, and negative and positive results. Insufficient metadata, lack of publicly available data, and incomplete information in study methods are considered to be the main reasons for poor reproducibility. The survey results also address a wide number of research questions on the reproducibility of scientific results. Based on the results of our explorative study and supported by the existing published literature, we offer general recommendations that could help the scientific community to understand, reproduce, and reuse experimental data and results in the research data lifecycle.},
  langid = {english},
  keywords = {TBR},
  file = {/Users/r.d.wit/Zotero/storage/XESQJIQV/Samuel and König-Ries - 2021 - Understanding experiments and research practices f.pdf}
}

@article{sandveTenSimpleRules2013,
  title = {Ten {{Simple Rules}} for {{Reproducible Computational Research}}},
  author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
  editor = {Bourne, Philip E.},
  year = {2013},
  month = oct,
  journal = {PLoS Computational Biology},
  volume = {9},
  number = {10},
  pages = {e1003285},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003285},
  urldate = {2022-05-09},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/H384NZXA/Sandve e.a. - 2013 - Ten Simple Rules for Reproducible Computational Re.pdf}
}

@article{sarikhaniMechanismsProvenanceCollection2018,
  title = {Mechanisms for Provenance Collection in Scientific Workflow Systems},
  author = {Sarikhani, Mehdi and Wendelborn, Andrew},
  year = {2018},
  month = may,
  journal = {Computing},
  volume = {100},
  number = {5},
  pages = {439--472},
  issn = {0010-485X, 1436-5057},
  doi = {10.1007/s00607-017-0578-1},
  urldate = {2022-05-15},
  langid = {english},
  keywords = {provenance,survey,workflow systems},
  file = {/Users/r.d.wit/Zotero/storage/VSXUN82Q/Sarikhani en Wendelborn - 2018 - Mechanisms for provenance collection in scientific.pdf}
}

@incollection{schultesReusableFAIRImplementation2020,
  title = {Reusable {{FAIR Implementation Profiles}} as {{Accelerators}} of {{FAIR Convergence}}},
  booktitle = {Advances in {{Conceptual Modeling}}},
  author = {Schultes, Erik and Magagna, Barbara and Hettne, Kristina Maria and Pergl, Robert and Such{\'a}nek, Marek and Kuhn, Tobias},
  editor = {Grossmann, Georg and Ram, Sudha},
  year = {2020},
  volume = {12584},
  pages = {138--147},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-65847-2_13},
  urldate = {2023-02-21},
  isbn = {978-3-030-65846-5 978-3-030-65847-2},
  langid = {english},
  keywords = {FAIR implementation profiles (FIP)},
  file = {/Users/r.d.wit/Zotero/storage/7TKQNKMD/Schultes et al. - 2020 - Reusable FAIR Implementation Profiles as Accelerat.pdf}
}

@article{sela-culangStructuralBasisAntibodyAntigen2013,
  title = {The {{Structural Basis}} of {{Antibody-Antigen Recognition}}},
  author = {{Sela-Culang}, Inbal and Kunik, Vered and Ofran, Yanay},
  year = {2013},
  journal = {Frontiers in Immunology},
  volume = {4},
  issn = {1664-3224},
  doi = {10.3389/fimmu.2013.00302},
  urldate = {2022-08-22},
  abstract = {The function of antibodies (Abs) involves specific binding to antigens (Ags) and activation of other components of the immune system to fight pathogens. The six hypervariable loops within the variable domains of Abs, commonly termed complementarity determining regions (CDRs), are widely assumed to be responsible for Ag recognition, while the constant domains are believed to mediate effector activation. Recent studies and analyses of the growing number of available Ab structures, indicate that this clear functional separation between the two regions may be an oversimplification. Some positions within the CDRs have been shown to never participate in Ag binding and some off-CDRs residues often contribute critically to the interaction with the Ag. Moreover, there is now growing evidence for non-local and even allosteric effects in Ab-Ag interaction in which Ag binding affects the constant region and vice versa. This review summarizes and discusses the structural basis of Ag recognition, elaborating on the contribution of different structural determinants of the Ab to Ag binding and recognition. We discuss the CDRs, the different approaches for their identification and their relationship to the Ag interface. We also review what is currently known about the contribution of non-CDRs regions to Ag recognition, namely the framework regions (FRs) and the constant domains. The suggested mechanisms by which these regions contribute to Ag binding are discussed. On the Ag side of the interaction, we discuss attempts to predict B-cell epitopes and the suggested idea to incorporate Ab information into B-cell epitope prediction schemes. Beyond improving the understanding of immunity, characterization of the functional role of different parts of the Ab molecule may help in Ab engineering, design of CDR-derived peptides, and epitope prediction.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/4NKTHMBR/Sela-Culang e.a. - 2013 - The Structural Basis of Antibody-Antigen Recogniti.pdf}
}

@article{silvelloTheoryPracticeData2018,
  title = {Theory and Practice of Data Citation},
  author = {Silvello, Gianmaria},
  year = {2018},
  month = jan,
  journal = {Journal of the Association for Information Science and Technology},
  volume = {69},
  number = {1},
  pages = {6--20},
  issn = {23301635},
  doi = {10.1002/asi.23917},
  urldate = {2022-04-21},
  abstract = {Citations are the cornerstone of knowledge propagation and the primary means of assessing the quality of research, as well as directing investments in science. Science is increasingly becoming ``data-intensive'', where large volumes of data are collected and analyzed to discover complex patterns through simulations and experiments, and most scientific reference works have been replaced by online curated datasets. Yet, given a dataset, there is no quantitative, consistent and established way of knowing how it has been used over time, who contributed to its curation, what results have been yielded or what value it has.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/4H5PLYBE/Silvello - 2018 - Theory and practice of data citation.pdf}
}

@article{smithSoftwareCitationPrinciples2016,
  title = {Software Citation Principles},
  author = {Smith, Arfon M. and Katz, Daniel S. and Niemeyer, Kyle E. and {FORCE11 Software Citation Working Group}},
  year = {2016},
  month = sep,
  journal = {PeerJ Computer Science},
  volume = {2},
  pages = {e86},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.86},
  urldate = {2022-04-19},
  abstract = {Software is a critical part of modern research and yet there is little support across the scholarly ecosystem for its acknowledgement and citation. Inspired by the activities of the FORCE11 working group focused on data citation, this document summarizes the recommendations of the FORCE11 Software Citation Working Group and its activities between June 2015 and April 2016. Based on a review of existing community practices, the goal of the working group was to produce a consolidated set of citation principles that may encourage broad adoption of a consistent policy for software citation across disciplines and venues. Our work is presented here as a set of software citation principles, a discussion of the motivations for developing the principles, reviews of existing community practice, and a discussion of the requirements these principles would place upon different stakeholders. Working examples and possible technical solutions for how these principles can be implemented will be discussed in a separate paper.},
  langid = {english},
  keywords = {software citation},
  file = {/Users/r.d.wit/Zotero/storage/HKMUCQIG/Smith e.a. - 2016 - Software citation principles.pdf}
}

@inproceedings{soiland-reyesArchivePackageArcp2018,
  title = {The {{Archive}} and {{Package}} (Arcp) {{URI Scheme}}},
  booktitle = {2018 {{IEEE}} 14th {{International Conference}} on E-{{Science}} (e-{{Science}})},
  author = {{Soiland-Reyes}, Stian and Caceres, Marcos},
  year = {2018},
  month = oct,
  pages = {40--44},
  publisher = {{IEEE}},
  address = {{Amsterdam}},
  doi = {10.1109/eScience.2018.00018},
  urldate = {2022-04-02},
  isbn = {978-1-5386-9156-4},
  langid = {english},
  keywords = {CWLProv,URI},
  file = {/Users/r.d.wit/Zotero/storage/KBDWREDC/Soiland-Reyes en Caceres - 2018 - The Archive and Package (arcp) URI Scheme.pdf}
}

@misc{soiland-reyesCommonworkflowlanguageCwlprovpy2022,
  title = {Common-Workflow-Language/Cwlprov-Py: 0.1.2},
  shorttitle = {Common-Workflow-Language/Cwlprov-Py},
  author = {{Soiland-Reyes}, Stian and Crusoe, Michael R. and Khan, Farah Zaib and Leinweber, Katrin},
  year = {2022},
  month = mar,
  doi = {10.5281/ZENODO.6334824},
  url = {https://doi.org/10.5281/ZENODO.6334824},
  urldate = {2023-05-20},
  abstract = {What's Changed Hyperlink DOI to preferred resolver by @katrinleinweber in https://github.com/common-workflow-language/cwlprov-py/pull/1 Upgrade to GitHub-native Dependabot by @dependabot-preview in https://github.com/common-workflow-language/cwlprov-py/pull/3 Modernize by @mr-c in https://github.com/common-workflow-language/cwlprov-py/pull/5 [Snyk] Security upgrade networkx from 2.2 to 2.6 by @snyk-bot in https://github.com/common-workflow-language/cwlprov-py/pull/6 Modernize codebase by @mr-c in https://github.com/common-workflow-language/cwlprov-py/pull/9 New Contributors @katrinleinweber made their first contribution in https://github.com/common-workflow-language/cwlprov-py/pull/1 @dependabot-preview made their first contribution in https://github.com/common-workflow-language/cwlprov-py/pull/3 @mr-c made their first contribution in https://github.com/common-workflow-language/cwlprov-py/pull/5 @snyk-bot made their first contribution in https://github.com/common-workflow-language/cwlprov-py/pull/6 {$<$}strong{$>$}Full Changelog{$<$}/strong{$>$}: https://github.com/common-workflow-language/cwlprov-py/compare/0.1.1...0.1.2},
  copyright = {Open Access},
  howpublished = {Zenodo}
}

@article{soiland-reyesPackagingResearchArtefacts2022,
  title = {Packaging Research Artefacts with {{RO-Crate}}},
  author = {{Soiland-Reyes}, Stian and Sefton, Peter and Crosas, Merc{\`e} and Castro, Leyla Jael and Coppens, Frederik and Fern{\'a}ndez, Jos{\'e} M. and Garijo, Daniel and Gr{\"u}ning, Bj{\"o}rn and La Rosa, Marco and Leo, Simone and {\'O} Carrag{\'a}in, Eoghan and Portier, Marc and Trisovic, Ana and {RO-Crate Community} and Groth, Paul and Goble, Carole},
  editor = {Peroni, Silvio},
  year = {2022},
  month = jan,
  journal = {Data Science},
  pages = {1--42},
  issn = {24518492, 24518484},
  doi = {10.3233/DS-210053},
  urldate = {2022-03-03},
  abstract = {An increasing number of researchers support reproducibility by including pointers to and descriptions of datasets, software and methods in their publications. However, scientific articles may be ambiguous, incomplete and difficult to process by automated systems. In this paper we introduce RO-Crate, an open, community-driven, and lightweight approach to packaging research artefacts along with their metadata in a machine readable manner. RO-Crate is based on Schema.org annotations in JSON-LD, aiming to establish best practices to formally describe metadata in an accessible and practical way for their use in a wide variety of situations.},
  langid = {english},
  keywords = {RO-Crate},
  file = {/Users/r.d.wit/Zotero/storage/JAAB3NIJ/Soiland-Reyes e.a. - 2022 - Packaging research artefacts with RO-Crate.pdf}
}

@misc{soiland-reyesWfdescOntology2016,
  title = {The {{Wfdesc Ontology}}},
  author = {{Soiland-Reyes}, Stian and Bechhofer, Sean and Corcho, Oscar and Belhajjame, Khalid and Garijo, Daniel and Garc{\'i}a Cuesta, Esteban and Palma, Raul},
  year = {2016},
  urldate = {2022-08-01},
  howpublished = {https://wf4ever.github.io/ro/2016-01-28/wfdesc/},
  keywords = {ontologies,ontology,provenance,wfdesc,workflows}
}

@misc{soiland-reyesWfprovOntology2016,
  title = {The {{Wfprov Ontology}}},
  author = {{Soiland-Reyes}, Stian and Bechhofer, Sean and Corcho, Oscar and Belhajjame, Khalid and Garijo, Daniel and Garc{\'i}a Cuesta, Esteban and Palma, Raul},
  year = {2016},
  urldate = {2022-08-01},
  howpublished = {https://wf4ever.github.io/ro/2016-01-28/wfprov/},
  keywords = {ontologies,ontology,provenance,wfprov}
}

@incollection{stamatogiannakisTradeOffsAutomaticProvenance2016,
  title = {Trade-{{Offs}} in {{Automatic Provenance Capture}}},
  booktitle = {Provenance and {{Annotation}} of {{Data}} and {{Processes}}},
  author = {Stamatogiannakis, Manolis and Kazmi, Hasanat and Sharif, Hashim and Vermeulen, Remco and Gehani, Ashish and Bos, Herbert and Groth, Paul},
  editor = {Mattoso, Marta and Glavic, Boris},
  year = {2016},
  volume = {9672},
  pages = {29--41},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-40593-3_3},
  urldate = {2022-05-15},
  abstract = {Automatic provenance capture from arbitrary applications is a challenging problem. Different approaches to tackle this problem have evolved, most notably a. system-event trace analysis, b. compile-time static instrumentation, and c. taint flow analysis using dynamic binary instrumentation. Each of these approaches offers different trade-offs in terms of the granularity of captured provenance, integration requirements, and runtime overhead. While these aspects have been discussed separately, a systematic and detailed study, quantifying and elucidating them, is still lacking. To fill this gap, we begin to explore these trade-offs for representative examples of these approaches for automatic provenance capture by means of evaluation and measurement. We base our evaluation on UnixBench\textemdash a widely used benchmark suite within systems research. We believe this approach will make our results easier to compare with future studies.},
  isbn = {978-3-319-40592-6 978-3-319-40593-3},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/MAKSET6B/Stamatogiannakis e.a. - 2016 - Trade-Offs in Automatic Provenance Capture.pdf}
}

@article{starrAchievingHumanMachine2015,
  title = {Achieving Human and Machine Accessibility of Cited Data in Scholarly Publications},
  author = {Starr, Joan and Castro, Eleni and Crosas, Merc{\`e} and Dumontier, Michel and Downs, Robert R. and Duerr, Ruth and Haak, Laurel L. and Haendel, Melissa and Herman, Ivan and Hodson, Simon and Hourcl{\'e}, Joe and Kratz, John Ernest and Lin, Jennifer and Nielsen, Lars Holm and Nurnberger, Amy and Proell, Stefan and Rauber, Andreas and Sacchi, Simone and Smith, Arthur and Taylor, Mike and Clark, Tim},
  year = {2015},
  month = may,
  journal = {PeerJ Computer Science},
  volume = {1},
  pages = {e1},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.1},
  urldate = {2022-04-22},
  abstract = {Reproducibility and reusability of research results is an important concern in scientific communication and science policy. A foundational element of reproducibility and reusability is the open and persistently available presentation of research data. However, many common approaches for primary data publication in use today do not achieve sufficient long-term robustness, openness, accessibility or uniformity.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/NQ6JS9ZS/Starr e.a. - 2015 - Achieving human and machine accessibility of cited.pdf}
}

@article{stevensClassificationTasksBioinformatics2001,
  title = {A Classification of Tasks in Bioinformatics},
  author = {Stevens, R. and Goble, C. and Baker, P. and Brass, A.},
  year = {2001},
  month = feb,
  journal = {Bioinformatics},
  volume = {17},
  number = {2},
  pages = {180--188},
  issn = {1367-4803, 1460-2059},
  doi = {10.1093/bioinformatics/17.2.180},
  urldate = {2022-03-07},
  abstract = {Motivation: This paper reports on a survey of bioinformatics tasks currently undertaken by working biologists. The aim was to find the range of tasks that need to be supported and the components needed to do this in a general query system. This enabled a set of evaluation criteria to be used to assess both the biology and mechanical nature of general query systems.},
  langid = {english},
  keywords = {workflows},
  file = {/Users/r.d.wit/Zotero/storage/XWGQZV5H/Stevens e.a. - 2001 - A classification of tasks in bioinformatics.pdf}
}

@article{stoddenEmpiricalAnalysisJournal2018,
  title = {An Empirical Analysis of Journal Policy Effectiveness for Computational Reproducibility},
  author = {Stodden, Victoria and Seiler, Jennifer and Ma, Zhaokun},
  year = {2018},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {11},
  pages = {2584--2589},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708290115},
  urldate = {2022-08-22},
  abstract = {A key component of scientific communication is sufficient information for other researchers in the field to reproduce published findings. For computational and data-enabled research, this has often been interpreted to mean making available the raw data from which results were generated, the computer code that generated the findings, and any additional information needed such as workflows and input parameters. Many journals are revising author guidelines to include data and code availability. This work evaluates the effectiveness of journal policy that requires the data and code necessary for reproducibility be made available postpublication by the authors upon request. We assess the effectiveness of such a policy by (               i               ) requesting data and code from authors and (               ii               ) attempting replication of the published findings. We chose a random sample of 204 scientific papers published in the journal               Science               after the implementation of their policy in February 2011. We found that we were able to obtain artifacts from 44\% of our sample and were able to reproduce the findings for 26\%. We find this policy\textemdash author remission of data and code postpublication upon request\textemdash an improvement over no policy, but currently insufficient for reproducibility.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/FMVNB95E/Stodden e.a. - 2018 - An empirical analysis of journal policy effectiven.pdf}
}

@inproceedings{stoddenEnablingVerificationComputational2018,
  title = {Enabling the {{Verification}} of {{Computational Results}}: {{An Empirical Evaluation}} of {{Computational Reproducibility}}},
  shorttitle = {Enabling the {{Verification}} of {{Computational Results}}},
  booktitle = {Proceedings of the {{First International Workshop}} on {{Practical Reproducible Evaluation}} of {{Computer Systems}}},
  author = {Stodden, Victoria and Krafczyk, Matthew S. and Bhaskar, Adhithya},
  year = {2018},
  month = jun,
  pages = {1--5},
  publisher = {{ACM}},
  address = {{Tempe AZ USA}},
  doi = {10.1145/3214239.3214242},
  urldate = {2022-08-22},
  abstract = {The ability to independently regenerate published computational claims is widely recognized as a key component of scientific reproducibility. In this article we take a narrow interpretation of this goal, and attempt to regenerate published claims from author-supplied information, including data, code, inputs, and other provided specifications, on a different computational system than that used by the original authors. We are motivated by Claerbout and Donoho's exhortation of the importance of providing complete information for reproducibility of the published claim. We chose the Elsevier journal, the Journal of Computational Physics, which has stated author guidelines that encourage the availability of computational digital artifacts that support scholarly findings. In an IRB approved study at the University of Illinois at Urbana-Champaign (IRB \#17329) we gathered artifacts from a sample of authors who published in this journal in 2016 and 2017. We then used the ICERM criteria generated at the 2012 ICERM workshop ``Reproducibility in Computational and Experimental Mathematics'' to evaluate the sufficiency of the information provided in the publications and the ease with which the digital artifacts afforded computational reproducibility. We find that, for the articles for which we obtained computational artifacts, we could not easily regenerate the findings for 67\% of them, and we were unable to easily regenerate all the findings for any of the articles. We then evaluated the artifacts we did obtain (55 of 306 articles) and find that the main barriers to computational reproducibility are inadequate documentation of code, data, and workflow information (70.9\%), missing code function and setting information, and missing licensing information (75\%). We recommend improvements based on these findings, including the deposit of supporting digital artifacts for reproducibility as a condition of publication, and verification of computational findings via reexecution of the code when possible.},
  isbn = {978-1-4503-5861-3},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/QJSRE74R/Stodden e.a. - 2018 - Enabling the Verification of Computational Results.pdf}
}

@article{stoddenEnhancingReproducibilityComputational2016,
  title = {Enhancing Reproducibility for Computational Methods},
  author = {Stodden, Victoria and McNutt, Marcia and Bailey, David H. and Deelman, Ewa and Gil, Yolanda and Hanson, Brooks and Heroux, Michael A. and Ioannidis, John P.A. and Taufer, Michela},
  year = {2016},
  month = dec,
  journal = {Science},
  volume = {354},
  number = {6317},
  pages = {1240--1241},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aah6168},
  urldate = {2022-08-05},
  abstract = {Data, code, and workflows should be available and cited           ,                             Over the past two decades, computational methods have radically changed the ability of researchers from all areas of scholarship to process and analyze data and to simulate complex systems. But with these advances come challenges that are contributing to broader concerns over irreproducibility in the scholarly literature, among them the lack of transparency in disclosure of computational methods. Current reporting methods are often uneven, incomplete, and still evolving. We present a novel set of Reproducibility Enhancement Principles (REP) targeting disclosure challenges involving computation. These recommendations, which build upon more general proposals from the Transparency and Openness Promotion (TOP) guidelines (                                1                              ) and recommendations for field data (                                2                              ), emerged from workshop discussions among funding agencies, publishers and journal editors, industry participants, and researchers representing a broad range of domains. Although some of these actions may be aspirational, we believe it is important to recognize and move toward ameliorating irreproducibility in computational research.},
  langid = {english},
  keywords = {provenance types,recommendations},
  file = {/Users/r.d.wit/Zotero/storage/GZZRKY4A/Stodden e.a. - 2016 - Enhancing reproducibility for computational method.pdf}
}

@article{stringerPIPENNProteinInterface2022,
  title = {{{PIPENN}}: Protein Interface Prediction from Sequence with an Ensemble of Neural Nets},
  shorttitle = {{{PIPENN}}},
  author = {Stringer, Bas and {de Ferrante}, Hans and Abeln, Sanne and Heringa, Jaap and Feenstra, K Anton and Haydarlou, Reza},
  editor = {Xu, Jinbo},
  year = {2022},
  month = apr,
  journal = {Bioinformatics},
  volume = {38},
  number = {8},
  pages = {2111--2118},
  issn = {1367-4803, 1460-2059},
  doi = {10.1093/bioinformatics/btac071},
  urldate = {2022-05-02},
  abstract = {Motivation: The interactions between proteins and other molecules are essential to many biological and cellular processes. Experimental identification of interface residues is a time-consuming, costly and challenging task, while protein sequence data are ubiquitous. Consequently, many computational and machine learning approaches have been developed over the years to predict such interface residues from sequence. However, the effectiveness of different Deep Learning (DL) architectures and learning strategies for protein\textendash protein, protein\textendash nucleotide and protein\textendash small molecule interface prediction has not yet been investigated in great detail. Therefore, we here explore the prediction of protein interface residues using six DL architectures and various learning strategies with sequencederived input features.},
  langid = {english},
  keywords = {NIAA workflow,PPI},
  file = {/Users/r.d.wit/Zotero/storage/PKIVU6I9/Stringer e.a. - 2022 - PIPENN protein interface prediction from sequence.pdf}
}

@article{thebiocondateamBiocondaSustainableComprehensive2018,
  title = {Bioconda: Sustainable and Comprehensive Software Distribution for the Life Sciences},
  shorttitle = {Bioconda},
  author = {{The Bioconda Team} and Gr{\"u}ning, Bj{\"o}rn and Dale, Ryan and Sj{\"o}din, Andreas and Chapman, Brad A. and Rowe, Jillian and {Tomkins-Tinch}, Christopher H. and Valieris, Renan and K{\"o}ster, Johannes},
  year = {2018},
  month = jul,
  journal = {Nature Methods},
  volume = {15},
  number = {7},
  pages = {475--476},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-018-0046-7},
  urldate = {2022-05-03},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/KISWNP6T/The Bioconda Team e.a. - 2018 - Bioconda sustainable and comprehensive software d.pdf}
}

@article{theuniprotconsortiumUniProtUniversalProtein2021,
  title = {{{UniProt}}: The Universal Protein Knowledgebase in 2021},
  shorttitle = {{{UniProt}}},
  author = {{The UniProt Consortium} and Bateman, Alex and Martin, Maria-Jesus and Orchard, Sandra and Magrane, Michele and Agivetova, Rahat and Ahmad, Shadab and Alpi, Emanuele and {Bowler-Barnett}, Emily H and Britto, Ramona and Bursteinas, Borisas and {Bye-A-Jee}, Hema and Coetzee, Ray and Cukura, Austra and Da Silva, Alan and Denny, Paul and Dogan, Tunca and Ebenezer, ThankGod and Fan, Jun and Castro, Leyla Garcia and Garmiri, Penelope and Georghiou, George and Gonzales, Leonardo and {Hatton-Ellis}, Emma and Hussein, Abdulrahman and Ignatchenko, Alexandr and Insana, Giuseppe and Ishtiaq, Rizwan and Jokinen, Petteri and Joshi, Vishal and Jyothi, Dushyanth and Lock, Antonia and Lopez, Rodrigo and Luciani, Aurelien and Luo, Jie and Lussi, Yvonne and MacDougall, Alistair and Madeira, Fabio and Mahmoudy, Mahdi and Menchi, Manuela and Mishra, Alok and Moulang, Katie and Nightingale, Andrew and Oliveira, Carla Susana and Pundir, Sangya and Qi, Guoying and Raj, Shriya and Rice, Daniel and Lopez, Milagros Rodriguez and Saidi, Rabie and Sampson, Joseph and Sawford, Tony and Speretta, Elena and Turner, Edward and Tyagi, Nidhi and Vasudev, Preethi and Volynkin, Vladimir and Warner, Kate and Watkins, Xavier and Zaru, Rossana and Zellner, Hermann and Bridge, Alan and Poux, Sylvain and Redaschi, Nicole and Aimo, Lucila and {Argoud-Puy}, Ghislaine and Auchincloss, Andrea and Axelsen, Kristian and Bansal, Parit and Baratin, Delphine and Blatter, Marie-Claude and Bolleman, Jerven and Boutet, Emmanuel and Breuza, Lionel and {Casals-Casas}, Cristina and {de Castro}, Edouard and Echioukh, Kamal Chikh and Coudert, Elisabeth and Cuche, Beatrice and Doche, Mikael and Dornevil, Dolnide and Estreicher, Anne and Famiglietti, Maria Livia and Feuermann, Marc and Gasteiger, Elisabeth and Gehant, Sebastien and Gerritsen, Vivienne and Gos, Arnaud and {Gruaz-Gumowski}, Nadine and Hinz, Ursula and Hulo, Chantal and {Hyka-Nouspikel}, Nevila and Jungo, Florence and Keller, Guillaume and Kerhornou, Arnaud and Lara, Vicente and Le Mercier, Philippe and Lieberherr, Damien and Lombardot, Thierry and Martin, Xavier and Masson, Patrick and Morgat, Anne and Neto, Teresa Batista and Paesano, Salvo and Pedruzzi, Ivo and Pilbout, Sandrine and Pourcel, Lucille and Pozzato, Monica and Pruess, Manuela and Rivoire, Catherine and Sigrist, Christian and Sonesson, Karin and Stutz, Andre and Sundaram, Shyamala and Tognolli, Michael and Verbregue, Laure and Wu, Cathy H and Arighi, Cecilia N and Arminski, Leslie and Chen, Chuming and Chen, Yongxing and Garavelli, John S and Huang, Hongzhan and Laiho, Kati and McGarvey, Peter and Natale, Darren A and Ross, Karen and Vinayaka, C R and Wang, Qinghua and Wang, Yuqi and Yeh, Lai-Su and Zhang, Jian and Ruch, Patrick and Teodoro, Douglas},
  year = {2021},
  month = jan,
  journal = {Nucleic Acids Research},
  volume = {49},
  number = {D1},
  pages = {D480-D489},
  issn = {0305-1048, 1362-4962},
  doi = {10.1093/nar/gkaa1100},
  urldate = {2022-02-17},
  abstract = {The aim of the UniProt Knowledgebase is to provide users with a comprehensive, high-quality and freely accessible set of protein sequences annotated with functional information. In this article, we describe significant updates that we have made over the last two years to the resource. The number of sequences in UniProtKB has risen to approximately 190 million, despite continued work to reduce sequence redundancy at the proteome level. We have adopted new methods of assessing proteome completeness and quality. We continue to extract detailed annotations from the literature to add to reviewed entries and supplement these in unreviewed entries with annotations provided by automated systems such as the newly implemented Association-Rule-Based Annotator (ARBA). We have developed a credit-based publication submission interface to allow the community to contribute publications and annotations to UniProt entries. We describe how UniProtKB responded to the COVID-19 pandemic through expert curation of relevant entries that were rapidly made available to the research community through a dedicated portal. UniProt resources are available under a CC-BY (4.0) license via the web at https://www.uniprot.org/.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/KD32USVS/The UniProt Consortium e.a. - 2021 - UniProt the universal protein knowledgebase in 20.pdf}
}

@TechReport{thew3csparqlworkinggroupSPARQLOverview2013,
 author = "{{The W3C SPARQL Working Group}}",
  title       = "{SPARQL} 1.1 Overview",
  month       = mar,
  url        = "https://www.w3.org/TR/2013/REC-sparql11-overview-20130321/",
  year        = "2013",
  bibsource   = "https://w2.syronex.com/jmr/w3c-biblio",
  type        = "{W3C} Recommendation",
  institution = "W3C",
}
@article{vanginkelPDBeCIFOpensourceMmCIF2021,
  title = {{{PDBeCIF}}: An Open-Source {{mmCIF}}/{{CIF}} Parsing and Processing Package},
  shorttitle = {{{PDBeCIF}}},
  author = {{van Ginkel}, Glen and Pravda, Luk{\'a}{\v s} and Dana, Jos{\'e} M. and Varadi, Mihaly and Keller, Peter and Anyango, Stephen and Velankar, Sameer},
  year = {2021},
  month = dec,
  journal = {BMC Bioinformatics},
  volume = {22},
  number = {1},
  pages = {383},
  issn = {1471-2105},
  doi = {10.1186/s12859-021-04271-9},
  urldate = {2022-04-08},
  abstract = {Background:\hspace{0.6em} Biomacromolecular structural data outgrew the legacy Protein Data Bank (PDB) format which the scientific community relied on for decades, yet the use of its successor PDBx/Macromolecular Crystallographic Information File format (PDBx/ mmCIF) is still not widespread. Perhaps one of the reasons is the availability of easy to use tools that only support the legacy format, but also the inherent difficulties of processing mmCIF files correctly, given the number of edge cases that make efficient parsing problematic. Nevertheless, to fully exploit macromolecular structure data and their associated annotations such as multiscale structures from integrative/hybrid methods or large macromolecular complexes determined using traditional methods, it is necessary to fully adopt the new format as soon as possible. Results:\hspace{0.6em} To this end, we developed PDBeCIF, an open-source Python project for manipulating mmCIF and CIF files. It is part of the official list of mmCIF parsers recorded by the wwPDB and is heavily employed in the processes of the Protein Data Bank in Europe. The package is freely available both from the PyPI repository (http:// pypi.org/project/pdbecif ) and from GitHub (https://github.com/pdbeurope/pdbecif ) along with rich documentation and many ready-to-use examples. Conclusions:\hspace{0.6em} PDBeCIF is an efficient and lightweight Python 2.6+/3+ package with no external dependencies. It can be readily integrated with 3rd party libraries as well as adopted for broad scientific analyses.},
  langid = {english},
  keywords = {epitope annotation,NIAA workflow},
  file = {/Users/r.d.wit/Zotero/storage/DJGZT6LH/van Ginkel e.a. - 2021 - PDBeCIF an open-source mmCIFCIF parsing and proc.pdf}
}

@inproceedings{versluisAnalysisWorkflowFormalisms2018,
  title = {An {{Analysis}} of {{Workflow Formalisms}} for {{Workflows}} with {{Complex Non-Functional Requirements}}},
  booktitle = {Companion of the 2018 {{ACM}}/{{SPEC International Conference}} on {{Performance Engineering}}},
  author = {Versluis, Laurens and {van Eyk}, Erwin and Iosup, Alexandru},
  year = {2018},
  month = apr,
  pages = {107--112},
  publisher = {{ACM}},
  address = {{Berlin Germany}},
  doi = {10.1145/3185768.3186297},
  urldate = {2022-05-12},
  abstract = {Cloud and datacenter operators offer progressively more sophisticated service level agreements to customers. The Quality-of-Service guarantees by these operators have started to entail non-functional requirements customers have regarding their applications. At the same time, expressing applications as workflows in datacenters is increasingly more common. Currently, non-functional requirements (NFRs) can only be defined on entire workflows and cannot be changed at runtime, possibly wasting valuable resources. To move towards modifiable NFRs at the task level, there is a need for a formalism capable of expressing this. Existing formalisms do not support this level of granularity or are restricted to a subset of NFRs. In this work, we investigate the current support for NFRs in existing formalisms. Using a library containing workflows with and without NFRs, we inspect the capability of existing formalisms to express these requirements. Additionally, we create and evaluate five metrics to qualitatively and quantitatively compare each formalism. Our main findings are that although current formalisms do not support arbitrary NFRs per-task, the Directed Acyclic Graphs (DAGs) formalism is the most suitable to extend.},
  isbn = {978-1-4503-5629-9},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/QVN62BXE/Versluis e.a. - 2018 - An Analysis of Workflow Formalisms for Workflows w.pdf}
}

@article{vivianToilEnablesReproducible2017,
  title = {Toil Enables Reproducible, Open Source, Big Biomedical Data Analyses},
  author = {Vivian, John and Rao, Arjun Arkal and Nothaft, Frank Austin and Ketchum, Christopher and Armstrong, Joel and Novak, Adam and Pfeil, Jacob and Narkizian, Jake and Deran, Alden D and {Musselman-Brown}, Audrey and Schmidt, Hannes and Amstutz, Peter and Craft, Brian and Goldman, Mary and Rosenbloom, Kate and Cline, Melissa and O'Connor, Brian and Hanna, Megan and Birger, Chet and Kent, W James and Patterson, David A and Joseph, Anthony D and Zhu, Jingchun and Zaranek, Sasha and Getz, Gad and Haussler, David and Paten, Benedict},
  year = {2017},
  month = apr,
  journal = {Nature Biotechnology},
  volume = {35},
  number = {4},
  pages = {314--316},
  issn = {1087-0156, 1546-1696},
  doi = {10.1038/nbt.3772},
  urldate = {2022-02-21},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/99PK36MS/Vivian e.a. - 2017 - Toil enables reproducible, open source, big biomed.pdf}
}

@article{wilkinsonFAIRGuidingPrinciples2016,
  title = {The {{FAIR Guiding Principles}} for Scientific Data Management and Stewardship},
  author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and {da Silva Santos}, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Merc{\`e} and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and {Gonzalez-Beltran}, Alejandra and Gray, Alasdair J.G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and {'t Hoen}, Peter A.C and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and {Rocca-Serra}, Philippe and Roos, Marco and {van Schaik}, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and {van der Lei}, Johan and {van Mulligen}, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
  year = {2016},
  month = dec,
  journal = {Scientific Data},
  volume = {3},
  number = {1},
  pages = {160018},
  issn = {2052-4463},
  doi = {10.1038/sdata.2016.18},
  urldate = {2022-05-09},
  langid = {english},
  keywords = {data citation,FAIR},
  file = {/Users/r.d.wit/Zotero/storage/U8GIYC83/Wilkinson e.a. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf}
}

@incollection{wittnerISO23494Biotechnology2021,
  title = {{{ISO}} 23494: {{Biotechnology}} \textendash{} {{Provenance Information Model}} for {{Biological Specimen And Data}}},
  shorttitle = {{{ISO}} 23494},
  booktitle = {Provenance and {{Annotation}} of {{Data}} and {{Processes}}},
  author = {Wittner, Rudolf and Holub, Petr and M{\"u}ller, Heimo and Geiger, Joerg and Goble, Carole and {Soiland-Reyes}, Stian and Pireddu, Luca and Frexia, Francesca and Mascia, Cecilia and Fairweather, Elliot and Swedlow, Jason R. and Moore, Josh and Strambio, Caterina and Grunwald, David and Nakae, Hiroki},
  editor = {Glavic, Boris and Braganholo, Vanessa and Koop, David},
  year = {2021},
  volume = {12839},
  pages = {222--225},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-80960-7_16},
  urldate = {2022-03-20},
  abstract = {Exchange of research data and samples in biomedical research has become a common phenomenon, demanding for their effective quality assessment. At the same time, several reports address reproducibility of research, where history of biological samples (acquisition, processing, transportation, storage, and retrieval) and data history (data generation and processing) define their fitness for purpose, and hence their quality. This project aims to develop a comprehensive W3C PROV based provenance information standard intended for the biomedical research domain. The standard is being developed by the working group 5 (''data processing and integration'') of the ISO (International Standardisation Organisation) technical committee 276 ``biotechnology''. The outcome of the project will be published in parts as international standards or technical specifications. The poster informs about the goals of the standardisation activity, presents the proposed structure of the standards, briefly describes its current state and outlines its future development and open issues.},
  isbn = {978-3-030-80959-1 978-3-030-80960-7},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/N4IESY5G/Wittner e.a. - 2021 - ISO 23494 Biotechnology – Provenance Information .pdf}
}

@article{wittnerLightweightDistributedProvenance2022,
  title = {Lightweight {{Distributed Provenance Model}} for {{Complex Real}}\textendash World {{Environments}}},
  author = {Wittner, Rudolf and Mascia, Cecilia and Gallo, Matej and Frexia, Francesca and M{\"u}ller, Heimo and Plass, Markus and Geiger, J{\"o}rg and Holub, Petr},
  year = {2022},
  month = aug,
  journal = {Scientific Data},
  volume = {9},
  number = {1},
  pages = {503},
  issn = {2052-4463},
  doi = {10.1038/s41597-022-01537-6},
  urldate = {2022-10-03},
  abstract = {Abstract             Provenance is information describing the lineage of an object, such as a dataset or biological material. Since these objects can be passed between organizations, each organization can document only parts of the objects life cycle. As a result, interconnection of distributed provenance parts forms distributed provenance chains. Dependant on the actual provenance content, complete provenance chains can provide traceability and contribute to reproducibility and FAIRness of research objects. In this paper, we define a lightweight provenance model based on W3C PROV that enables generation of distributed provenance chains in complex, multi-organizational environments. The application of the model is demonstrated with a use case spanning several steps of a real-world research pipeline \textemdash{} starting with the acquisition of a specimen, its processing and storage, histological examination, and the generation/collection of associated data (images, annotations, clinical data), ending with training an AI model for the detection of tumor in the images. The proposed model has become an open conceptual foundation of the currently developed ISO 23494 standard on provenance for biotechnology domain.},
  langid = {english},
  keywords = {Open Provenance Model,TBR},
  file = {/Users/r.d.wit/Zotero/storage/DNQ29URL/Wittner e.a. - 2022 - Lightweight Distributed Provenance Model for Compl.pdf}
}

@article{xuOPUSTASSProteinBackbone2020,
  title = {{{OPUS-TASS}}: A Protein Backbone Torsion Angles and Secondary Structure Predictor Based on Ensemble Neural Networks},
  shorttitle = {{{OPUS-TASS}}},
  author = {Xu, Gang and Wang, Qinghua and Ma, Jianpeng},
  editor = {Elofsson, Arne},
  year = {2020},
  month = dec,
  journal = {Bioinformatics},
  volume = {36},
  number = {20},
  pages = {5021--5026},
  issn = {1367-4803, 1460-2059},
  doi = {10.1093/bioinformatics/btaa629},
  urldate = {2022-03-03},
  abstract = {Motivation: Predictions of protein backbone torsion angles (/ and w) and secondary structure from sequence are crucial subproblems in protein structure prediction. With the development of deep learning approaches, their accuracies have been significantly improved. To capture the long-range interactions, most studies integrate bidirectional recurrent neural networks into their models. In this study, we introduce and modify a recently proposed architecture named Transformer to capture the interactions between the two residues theoretically with arbitrary distance. Moreover, we take advantage of multitask learning to improve the generalization of neural network by introducing related tasks into the training process. Similar to many previous studies, OPUS-TASS uses an ensemble of models and achieves better results.},
  langid = {english},
  file = {/Users/r.d.wit/Zotero/storage/C2X3Y3WX/Xu e.a. - 2020 - OPUS-TASS a protein backbone torsion angles and s.pdf}
}

@article{yilmazMinimumInformationMarker2011,
  title = {Minimum Information about a Marker Gene Sequence ({{MIMARKS}}) and Minimum Information about Any (x) Sequence ({{MIxS}}) Specifications},
  author = {Yilmaz, Pelin and Kottmann, Renzo and Field, Dawn and Knight, Rob and Cole, James R and {Amaral-Zettler}, Linda and Gilbert, Jack A and {Karsch-Mizrachi}, Ilene and Johnston, Anjanette and Cochrane, Guy and Vaughan, Robert and Hunter, Christopher and Park, Joonhong and Morrison, Norman and {Rocca-Serra}, Philippe and Sterk, Peter and Arumugam, Manimozhiyan and Bailey, Mark and Baumgartner, Laura and Birren, Bruce W and Blaser, Martin J and Bonazzi, Vivien and Booth, Tim and Bork, Peer and Bushman, Frederic D and Buttigieg, Pier Luigi and Chain, Patrick S G and Charlson, Emily and Costello, Elizabeth K and {Huot-Creasy}, Heather and Dawyndt, Peter and DeSantis, Todd and Fierer, Noah and Fuhrman, Jed A and Gallery, Rachel E and Gevers, Dirk and Gibbs, Richard A and Gil, Inigo San and Gonzalez, Antonio and Gordon, Jeffrey I and Guralnick, Robert and Hankeln, Wolfgang and Highlander, Sarah and Hugenholtz, Philip and Jansson, Janet and Kau, Andrew L and Kelley, Scott T and Kennedy, Jerry and Knights, Dan and Koren, Omry and Kuczynski, Justin and Kyrpides, Nikos and Larsen, Robert and Lauber, Christian L and Legg, Teresa and Ley, Ruth E and Lozupone, Catherine A and Ludwig, Wolfgang and Lyons, Donna and Maguire, Eamonn and Meth{\'e}, Barbara A and Meyer, Folker and Muegge, Brian and Nakielny, Sara and Nelson, Karen E and Nemergut, Diana and Neufeld, Josh D and Newbold, Lindsay K and Oliver, Anna E and Pace, Norman R and Palanisamy, Giriprakash and Peplies, J{\"o}rg and Petrosino, Joseph and Proctor, Lita and Pruesse, Elmar and Quast, Christian and Raes, Jeroen and Ratnasingham, Sujeevan and Ravel, Jacques and Relman, David A and {Assunta-Sansone}, Susanna and Schloss, Patrick D and Schriml, Lynn and Sinha, Rohini and Smith, Michelle I and Sodergren, Erica and Spor, Aym{\'e} and Stombaugh, Jesse and Tiedje, James M and Ward, Doyle V and Weinstock, George M and Wendel, Doug and White, Owen and Whiteley, Andrew and Wilke, Andreas and Wortman, Jennifer R and Yatsunenko, Tanya and Gl{\"o}ckner, Frank Oliver},
  year = {2011},
  month = may,
  journal = {Nature Biotechnology},
  volume = {29},
  number = {5},
  pages = {415--420},
  issn = {1087-0156, 1546-1696},
  doi = {10.1038/nbt.1823},
  urldate = {2023-03-17},
  langid = {english},
  keywords = {BioSB Knowledge Graphs 2023 (WUR),Jasper Koehorst,metadata},
  file = {/Users/r.d.wit/Zotero/storage/X36TWHN9/Yilmaz et al. - 2011 - Minimum information about a marker gene sequence (.pdf}
}
